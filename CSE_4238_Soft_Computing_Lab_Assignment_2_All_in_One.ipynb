{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSE 4238 - Soft Computing Lab - Assignment 2- All in One.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "LVJMNAzd6cyj",
        "yVEH4KHhjoC9",
        "and1qRpqjrrG",
        "K1W3UGlL5h96",
        "I6btXeeG5cLi",
        "D5AWPysr52J6",
        "Qzg5GVjV555R",
        "nH_5Fjucw7Vs",
        "v1Q9jiYY6SwS",
        "CZokUlmS-QlQ",
        "Muzux96C5B4p",
        "ITcWE8G--CQq",
        "vu-fq2VRtEWs",
        "WUibX4ou1S6g",
        "3OMDfjb93LSi",
        "u77XBDPsK_Y7",
        "PH-sgSiwfVjc",
        "z4ZOi5mGuLcD",
        "tFMzERf9EoFP"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rafat97/bengali-handwritten-digits-0-to-9-classification/blob/master/CSE_4238_Soft_Computing_Lab_Assignment_2_All_in_One.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EYVTkeU6hmy"
      },
      "source": [
        "# Notebook Author info\n",
        "<center>\n",
        "\n",
        "| |  |\n",
        "| ----------- | ----------- |\n",
        "| ![Emdadul Haque Rafat](https://rafat97.github.io/static/c3688eb99d1fef50023a121e3abc5fa6/e8044/my-image.jpg)      | `Name:` Emdadul Haque<br /><br /> `Professional Status:` Student of Computer Science and Engineering <br /><br /> `Email:` rafathaque1997@gmail.com <br /><br /> `Website :` https://rafat97.github.io/ <br /><br />`Github:` https://github.com/Rafat97 <br /><br /> `Linkedin:` https://www.linkedin.com/in/rafat-haque-173131139/   |\n",
        "\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVJMNAzd6cyj"
      },
      "source": [
        "# Drive mount code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP2_K3IB3rHL"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVEH4KHhjoC9"
      },
      "source": [
        "# Dataset Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXSLNONRQYW4"
      },
      "source": [
        "# import some importent library or packages \n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import time,sys\n",
        "import copy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import pathlib\n",
        "import zipfile\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "import torch.optim as optim\n",
        "import pathlib\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "!pip install torchviz\n",
        "from torchviz import make_dot, make_dot_from_trace\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpcmFapg4GQC"
      },
      "source": [
        "!gdown --id 1txyKhs1Zt5AKswGGK9VI_jE0JNHuQT85"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gug3bvwK4IpM"
      },
      "source": [
        "!unzip '/content/Dataset A.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "and1qRpqjrrG"
      },
      "source": [
        "## Read `csv` file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXmL0p_TrEfO"
      },
      "source": [
        "!rm -rf '/content/PROCESSED_DATASET-170104028'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14wgV0kKi6zP"
      },
      "source": [
        "traning_csv = '/content/training-a.csv'\n",
        "read_df = pd.read_csv(traning_csv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imUxJA6lmdj2"
      },
      "source": [
        "read_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZiqjtWFykiz"
      },
      "source": [
        "number_of_digit_class = 10\n",
        "for i in range(0,number_of_digit_class):\n",
        "  select_digit = read_df[read_df['digit'] == i]\n",
        "  print(i , select_digit.shape )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1W3UGlL5h96"
      },
      "source": [
        "## Dataset processed & store into `digit` based folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyUwXfEkjQoP"
      },
      "source": [
        "number_of_digit_class = read_df['digit'].nunique()\n",
        "\n",
        "DATASET_ROOT_DIR = './PROCESSED_DATASET_170104028/' \n",
        "path = Path(DATASET_ROOT_DIR)\n",
        "path.mkdir(parents=True, exist_ok=True)\n",
        "DATASET_ROOT_DIR = os.path.abspath(path)\n",
        "\n",
        "for i in range(0,number_of_digit_class):\n",
        "  select_digit = read_df[read_df['digit'] == i]\n",
        "  for index,val in select_digit.iterrows():\n",
        "    file_relative_path_from= f\"./{val['database name']}/{val['filename']}\"\n",
        "    file_relative_dir_to = f\"{DATASET_ROOT_DIR}/{val['digit']}\"\n",
        "    Path(file_relative_dir_to).mkdir(parents=True, exist_ok=True)\n",
        "    shutil.copy(file_relative_path_from, file_relative_dir_to)\n",
        "    print(file_relative_path_from , file_relative_dir_to , \"OK\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6btXeeG5cLi"
      },
      "source": [
        "## Check Image is correct"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO3fbVc9xIrv"
      },
      "source": [
        "#@title #<center>All images are correct or not using pytorch `ImageFolder`</center> { display-mode: \"form\" }\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "        # transforms.RandomResizedCrop(256),\n",
        "        # transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "      ])\n",
        "\n",
        "img_path = '/content/PROCESSED_DATASET-170104028'  #@param {type: \"string\"}\n",
        "train_dataset=datasets.ImageFolder(root=img_path,transform=train_transforms)\n",
        "print( train_dataset.class_to_idx )\n",
        "dataloaders = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=False,pin_memory=True)\n",
        "for image,label in dataloaders:\n",
        "\n",
        "  plt.figure(figsize=(20,20))\n",
        "  grid_imge_gen = torchvision.utils.make_grid(image)\n",
        "  plt.imshow(grid_imge_gen.permute(1, 2, 0).cpu())\n",
        "  plt.show()\n",
        "  print(label)\n",
        "  # break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5AWPysr52J6"
      },
      "source": [
        "## Create dataset zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziguq0Fxywtb"
      },
      "source": [
        "ZIP_FILE_NAME = 'PROCESSED_DATASET-170104028.zip'\n",
        "!zip -r $ZIP_FILE_NAME 'PROCESSED_DATASET-170104028/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qzg5GVjV555R"
      },
      "source": [
        "## Copy the zip file into drive or safe place "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A801vVA3hF0"
      },
      "source": [
        "!cp '/content/PROCESSED_DATASET-170104028.zip'  '/content/drive/MyDrive/Datasets'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH_5Fjucw7Vs"
      },
      "source": [
        "# Data Load From zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlUs8I-8bvII"
      },
      "source": [
        "# import some importent library or packages \n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import time,sys\n",
        "import copy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import pathlib\n",
        "import zipfile\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "import torch.optim as optim\n",
        "import pathlib\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "!pip install torchviz\n",
        "from torchviz import make_dot, make_dot_from_trace\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "torch.manual_seed(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaKNziom4vdo"
      },
      "source": [
        "## Download preprocessed dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyR7nygy4chy"
      },
      "source": [
        "!gdown --id '1cml-H4UUJyY0hRoVeAynO8JjvWqa7FU1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMKBFffe458J"
      },
      "source": [
        "## Unzip the download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4KGVLdE4qXK"
      },
      "source": [
        "!unzip '/content/PROCESSED_DATASET-170104028.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQQQa_d35TTZ"
      },
      "source": [
        "## Remove the zip file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyZ6yOMA5PQJ"
      },
      "source": [
        "!rm '/content/PROCESSED_DATASET-170104028.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1Q9jiYY6SwS"
      },
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMbUWiw3Nj9J"
      },
      "source": [
        "# import some importent library or packages \n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import time,sys\n",
        "import copy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import pathlib\n",
        "import zipfile\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "import torch.optim as optim\n",
        "import pathlib\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "!pip install torchviz\n",
        "from torchviz import make_dot, make_dot_from_trace\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "torch.manual_seed(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZokUlmS-QlQ"
      },
      "source": [
        "# important variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVsnfKIp-P-Q"
      },
      "source": [
        "base_dir = '/content/PROCESSED_DATASET-170104028' \n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# 28(model-1), 128(model-2), 224(model-3),\n",
        "IMAGE_SIZE = 224\n",
        "LEARNING_RATE = 0.01\n",
        "TEST_SIZE = 0.2\n",
        "OUTPUT_DIM=10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Muzux96C5B4p"
      },
      "source": [
        "# Load Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLPbZAzwN0Vs"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "                                # transforms.ToPILImage(),\n",
        "\n",
        "                                ## this is only for when model is 1\n",
        "                                # transforms.Grayscale(), \n",
        "                                \n",
        "                                # transforms.RandomRotation(20,expand=True), ## adding random rotation 20deg\n",
        "                                # torchvision.transforms.ColorJitter(hue=.05, saturation=.05), ## adding color filter\n",
        "                                # transforms.RandomVerticalFlip(), ## adding vertical flip\n",
        "                                # transforms.RandomHorizontalFlip(), ## adding horizontal flip\n",
        "                                transforms.Resize(IMAGE_SIZE),  ## image resize\n",
        "                                transforms.CenterCrop(IMAGE_SIZE), ## image center crop\n",
        "                                transforms.ToTensor(), ## array converted into torch tensor and then divided by 255 (1.0/255)\n",
        "                                # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                               ])\n",
        "\n",
        "#load training dataset \n",
        "dataset = torchvision.datasets.ImageFolder(base_dir, transform=transform)\n",
        "n = len(dataset) \n",
        "n_test = int(TEST_SIZE * n) # 10% validation\n",
        "trainDataset, validDataSet = torch.utils.data.random_split(dataset,[n - n_test,n_test]) #random split dataset\n",
        "trainloader = torch.utils.data.DataLoader(trainDataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,) \n",
        "validationloader = torch.utils.data.DataLoader(validDataSet, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,) \n",
        "print(\"Length of the trainloader:\", len(trainloader ) * BATCH_SIZE)\n",
        "print(\"Length of the validationloader:\", len(validationloader ) * BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Svy95MXy_5BB"
      },
      "source": [
        "\n",
        "# trainDataset = torchvision.datasets.FashionMNIST(root='./data', \n",
        "#                             train=True, \n",
        "#                             transform=transforms.ToTensor(),  # Normalize the image to [0-1] from [0-255]\n",
        "#                             download=True)\n",
        "\n",
        "# validDataSet = torchvision.datasets.FashionMNIST(root='./data', \n",
        "#                            train=False, \n",
        "#                            transform=transforms.ToTensor())\n",
        "\n",
        "# '''\n",
        "# MAKING DATASET ITERABLE\n",
        "# '''\n",
        "\n",
        "# trainloader = torch.utils.data.DataLoader(dataset=trainDataset, \n",
        "#                                            batch_size=BATCH_SIZE, \n",
        "#                                            shuffle=True)   # It's better to shuffle the whole training dataset! \n",
        "\n",
        "# validationloader = torch.utils.data.DataLoader(dataset=validDataSet, \n",
        "#                                           batch_size=BATCH_SIZE, \n",
        "#                                           shuffle=False)  \n",
        "\n",
        "# print(\"Length of the trainloader:\", len(trainloader ) * BATCH_SIZE)\n",
        "# print(\"Length of the validationloader:\", len(validationloader ) * BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITcWE8G--CQq"
      },
      "source": [
        "# Data basic info."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah_Uya58td5d"
      },
      "source": [
        "dataset.class_to_idx # class map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMKQzdDG7NZ5"
      },
      "source": [
        "print(dict(Counter(dataset.targets))) # count number of data in a class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM9GI3D17tUp"
      },
      "source": [
        "train_classes = [label for _, label in trainDataset]\n",
        "print(dict(Counter(train_classes)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4u7UWJl858x"
      },
      "source": [
        "val_classes = [label for _, label in validDataSet]\n",
        "print(dict(Counter(val_classes)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8e6dxJq-dwY"
      },
      "source": [
        "for image,label in trainloader:\n",
        "  plt.figure(figsize=(20,20))\n",
        "  grid_imge_gen = torchvision.utils.make_grid(image)\n",
        "  plt.imshow(grid_imge_gen.permute(1, 2, 0).cpu())\n",
        "  plt.title(\"Trainloader\")\n",
        "  plt.show()\n",
        "  print(label)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAnMIFYL-opY"
      },
      "source": [
        "for image,label in validationloader:\n",
        "  plt.figure(figsize=(20,20))\n",
        "  grid_imge_gen = torchvision.utils.make_grid(image)\n",
        "  plt.imshow(grid_imge_gen.permute(1, 2, 0).cpu())\n",
        "  plt.title(\"Validationloader\")\n",
        "  plt.show()\n",
        "  print(label)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpJj9oCT_wEx"
      },
      "source": [
        "# Model Creation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-tEKJ2uV8K0"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu-fq2VRtEWs"
      },
      "source": [
        "## Model 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13hvC1cTV6AN"
      },
      "source": [
        "'''\n",
        "Model creation \n",
        "'''\n",
        "\n",
        "class LIN_MODEL(torch.nn.Module): \n",
        "    def __init__(self,outDim):\n",
        "        super(LIN_MODEL, self).__init__()\n",
        "\n",
        "        self.fc_1 = torch.nn.Linear(784, 90)\n",
        "        self.fc_2 = torch.nn.Linear(90, 50)\n",
        "        self.fc_3 = torch.nn.Linear(50, 30)\n",
        "        self.fc_4 = torch.nn.Linear(30, 18)\n",
        "        self.fc_5 = torch.nn.Linear(18, 12)\n",
        "        self.fc_6 = torch.nn.Linear(12, outDim)\n",
        "\n",
        "        self.linear = torch.nn.Linear(784, outDim)\n",
        " \n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(-1, 28 * 28) \n",
        "        x = torch.nn.functional.relu(self.fc_1(x))\n",
        "        x = torch.nn.functional.softmax(self.fc_2(x),dim=1)\n",
        "        x = torch.nn.functional.relu(self.fc_3(x))\n",
        "        x = torch.nn.functional.softmax(self.fc_4(x),dim=1)\n",
        "        x = torch.nn.functional.relu(self.fc_5(x))\n",
        "        x = self.fc_6(x)\n",
        "        # x = torch.nn.functional.softmax(self.linear(x),dim=1)\n",
        "        return x\n",
        "        \n",
        "\n",
        "model_1 = LIN_MODEL(OUTPUT_DIM).to(device)\n",
        "\n",
        "summary( model_1, input_size=(1, 28, 28))\n",
        "# select CPU or GPU as a device\n",
        "print(model_1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykGaF2ILrB-H"
      },
      "source": [
        "x = torch.randn(BATCH_SIZE,1,28,28).to(device)\n",
        "make_dot(model_1(x), params=dict(model_1.named_parameters()), show_attrs=True, show_saved=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d65L4VNBmWY"
      },
      "source": [
        "make_dot(model_1(x), params=dict(model_1.named_parameters()), show_attrs=True, show_saved=True).render(\"Model-1\", format=\"png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lKpdU7gtOcS"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_1.parameters(),lr=LEARNING_RATE, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUibX4ou1S6g"
      },
      "source": [
        "## Model 1 v2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX3e6Zyc1WkH"
      },
      "source": [
        "class LIN_MODEL_2(torch.nn.Module): \n",
        "    def __init__(self,outDim):\n",
        "        super(LIN_MODEL_2, self).__init__()\n",
        "\n",
        "        self.fc_1 = torch.nn.Linear(784, 742)\n",
        "        self.fc_2 = torch.nn.Linear(742, 621)\n",
        "        self.fc_3 = torch.nn.Linear(621, 510)\n",
        "        self.fc_6 = torch.nn.Linear(510, outDim)\n",
        " \n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(-1, 28 * 28) \n",
        "        x = torch.nn.functional.relu(self.fc_1(x))\n",
        "        x = torch.nn.functional.relu(self.fc_2(x))\n",
        "        x = torch.nn.functional.relu(self.fc_3(x))\n",
        "        x = torch.nn.functional.relu(self.fc_6(x))\n",
        "\n",
        "        return x\n",
        "        \n",
        "model_1_2 = LIN_MODEL_2(OUTPUT_DIM).to(device)\n",
        "\n",
        "summary( model_1_2, input_size=(1, 28, 28))\n",
        "# select CPU or GPU as a device\n",
        "print(model_1_2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAN4CWc21mED"
      },
      "source": [
        "x = torch.randn(BATCH_SIZE,1,28,28).to(device)\n",
        "make_dot(model_1_2(x), params=dict(model_1_2.named_parameters()), show_attrs=True, show_saved=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3FOVAipBilb"
      },
      "source": [
        "make_dot(model_1_2(x), params=dict(model_1_2.named_parameters()), show_attrs=True, show_saved=True).render(\"Model-1-2\", format=\"png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoTH_ZBv1skg"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_1_2.parameters(),lr=LEARNING_RATE, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OMDfjb93LSi"
      },
      "source": [
        "## Model 1 V3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsaXBHQ83K1m"
      },
      "source": [
        "class LIN_MODEL_3(torch.nn.Module): \n",
        "    def __init__(self,outDim):\n",
        "        super(LIN_MODEL_3, self).__init__()\n",
        "\n",
        "        self.fc_1 = torch.nn.Linear(784, 512)\n",
        "        self.fc_2 = torch.nn.Linear(512, 256)\n",
        "        self.fc_6 = torch.nn.Linear(256, outDim)\n",
        " \n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(-1, 28 * 28) \n",
        "        x = torch.nn.functional.relu(self.fc_1(x))\n",
        "        x = torch.nn.functional.relu(self.fc_2(x))\n",
        "        x = torch.nn.functional.relu(self.fc_6(x))\n",
        "\n",
        "        return x\n",
        "        \n",
        "model_1_3 = LIN_MODEL_3(OUTPUT_DIM).to(device)\n",
        "\n",
        "summary( model_1_3, input_size=(1, 28, 28))\n",
        "print(model_1_3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYyk1_LZ3cNP"
      },
      "source": [
        "x = torch.randn(BATCH_SIZE,1,28,28).to(device)\n",
        "make_dot(model_1_3(x), params=dict(model_1_3.named_parameters()), show_attrs=True, show_saved=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRV0Yc-X3hmS"
      },
      "source": [
        "make_dot(model_1_3(x), params=dict(model_1_3.named_parameters()), show_attrs=True, show_saved=True).render(\"Model-1-3\", format=\"png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAcK1jAI3nwi"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_1_3.parameters(),lr=LEARNING_RATE, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u77XBDPsK_Y7"
      },
      "source": [
        "## Model 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BNNndDmK_I5"
      },
      "source": [
        "class CNN(torch.nn.Module): \n",
        "    def __init__(self ,outDim):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        #initializing convolution layer\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3)\n",
        "        self.conv2 = torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n",
        "\n",
        "        #initializing dropout \n",
        "        self.dropout = torch.nn.Dropout(0.2)\n",
        "\n",
        "        #initializing dropout \n",
        "        self.pool= torch.nn.MaxPool2d(2,2)\n",
        " \n",
        "        #initializing linear\n",
        "        self.fc1 = torch.nn.Linear(32* 30* 30, 512)\n",
        "        self.fc2 = torch.nn.Linear(512,64)\n",
        "        self.fc3 = torch.nn.Linear(64,10)\n",
        "\n",
        " \n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.nn.functional.relu(self.conv1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.pool(torch.nn.functional.relu(self.conv2(x))) \n",
        "        x = self.dropout(x)\n",
        "        x = x.view(-1, 32* 30* 30) \n",
        "        # print(x.shape)\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        x = torch.nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "        \n",
        "\n",
        "model_2 = CNN(OUTPUT_DIM).to(device)\n",
        "\n",
        "summary( model_2, input_size=(3, 128, 128))\n",
        "# select CPU or GPU as a device\n",
        "print(model_2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LXmRQkmLX1E"
      },
      "source": [
        "x = torch.randn(1,3,128,128).to(device)\n",
        "make_dot(model_2(x), params=dict(model_2.named_parameters()), show_attrs=True, show_saved=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inF2IqPsahxB"
      },
      "source": [
        "make_dot(model_2(x), params=dict(model_2.named_parameters()), show_attrs=True, show_saved=True).render(\"Model-2-CNN\", format=\"png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVOSizzEyt04"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_2.parameters(),lr=LEARNING_RATE, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH-sgSiwfVjc"
      },
      "source": [
        "## Model 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q23zLm-RRir9"
      },
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "      \n",
        "num_classes = OUTPUT_DIM\n",
        "\n",
        "\n",
        "model_3 = models.resnet152(pretrained=True)\n",
        "set_parameter_requires_grad(model_3, True)\n",
        "num_ftrs = model_3.fc.in_features\n",
        "model_3.fc = torch.nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "model_3 = model_3.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9dSDf63ao2p"
      },
      "source": [
        "x = torch.randn(1,3,224,224).to(device)\n",
        "make_dot(model_3(x), params=dict(model_3.named_parameters()), show_attrs=True, show_saved=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h25Fz-2dapNZ"
      },
      "source": [
        "make_dot(model_3(x), params=dict(model_3.named_parameters()), show_attrs=True, show_saved=True).render(\"Model-3-RSNET-152\", format=\"png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBIOrzUaMqgB"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_3.parameters(),lr=LEARNING_RATE, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dbj7OlK0EKb9"
      },
      "source": [
        "# Start Tranning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4ZOi5mGuLcD"
      },
      "source": [
        "## Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py448GSDuJTL"
      },
      "source": [
        "def save_model(\n",
        "               MODEL_USED,\n",
        "               SAVEPATH,\n",
        "               epoch, \n",
        "               batch_size, \n",
        "               model,\n",
        "               optimizer,\n",
        "               image_size,\n",
        "               tranning_loss=[],\n",
        "               tranning_acc=[],\n",
        "               validation_loss=[],\n",
        "               validation_acc=[],\n",
        "               learning_rate=0.001,\n",
        "               meta_data=None):\n",
        "  SAVEPATH += f\"{MODEL_USED}-checkpoint-epoch-{epoch}.pt\"\n",
        "  save_obj = {\n",
        "       'MODEL_USED':MODEL_USED,\n",
        "       'batch_size':batch_size,\n",
        "       'epoch': epoch,\n",
        "       'model_full': model,\n",
        "       'optimizer_full': optimizer,\n",
        "       'model_state': model.state_dict(),\n",
        "       'optimizer_state': optimizer.state_dict(),\n",
        "       'image_size': image_size,\n",
        "       'tranning_loss': tranning_loss,\n",
        "       'tranning_acc': tranning_acc,\n",
        "       'validation_loss': validation_loss,\n",
        "       'validation_acc': validation_acc,\n",
        "       'learning_rate':learning_rate,\n",
        "       'meta_data':meta_data\n",
        "       }\n",
        "\n",
        "  torch.save(save_obj, SAVEPATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w0oLNMdu0Jr"
      },
      "source": [
        "## Train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjWXGOHiuTnr"
      },
      "source": [
        "def train_model(start, end, \n",
        "                model_used ,\n",
        "                model_save_path, \n",
        "                model, \n",
        "                criterion, \n",
        "                optimizer, \n",
        "                dataloaders,\n",
        "                testloaders , \n",
        "                lernRate=0.001,\n",
        "                all_tranning_loss=[], all_validation_loss=[], all_tranning_accuracy=[], all_validation_accuracy=[]):\n",
        "    since = time.time()\n",
        "    num_epochs = end\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    phase = 'train'\n",
        "    steps = 0\n",
        "\n",
        "    for epoch in range(start,num_epochs):\n",
        "          model.train()\n",
        "          phase = 'tranning'\n",
        "          print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "          print('-' * 10)\n",
        "          running_loss = 0.0\n",
        "          running_corrects = 0\n",
        "\n",
        "          for i,(inputs, labels) in enumerate(dataloaders):\n",
        "                \n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # running_loss += loss.item() * inputs.size(0)\n",
        "                running_loss += loss.item()\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                \n",
        "\n",
        "                print_val = f\"Epoch: {epoch}/{num_epochs-1} Steps {steps} \\t\"\n",
        "                print_val += f\"running_loss : {(loss.item()):.6f}\\t\"\n",
        "                print_val += f\"running_corrects : {torch.sum(preds == labels.data)}\\t\"  \n",
        "                print_val += f\"total_corrects : {running_corrects}\\t\"  \n",
        "                sys.stdout.write('\\r' + str(print_val))\n",
        "                steps += 1\n",
        "          \n",
        "          \n",
        "          steps = 0\n",
        "          epoch_loss = running_loss / len(dataloaders)\n",
        "          epoch_acc = running_corrects.double().item() /len(dataloaders.dataset)\n",
        "          all_tranning_loss.append(loss.item())\n",
        "          all_tranning_accuracy.append(epoch_acc)\n",
        "          \n",
        "          \n",
        "          print(\"\\n\")\n",
        "          print(\"----------------------------Tranning Summary----------------------\")\n",
        "          print('{} Tranning Avg. Loss: {:.4f} Tranning Avg. Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "          print(\"-\"*70)\n",
        "          print(\"\\n\")\n",
        "\n",
        "          print(\"Start Validation\")\n",
        "          model.eval()\n",
        "          with torch.no_grad():\n",
        "              phase = \"validation\"\n",
        "              running_loss = 0.0\n",
        "              running_corrects = 0\n",
        "              for i,(inputs, labels) in enumerate(testloaders):\n",
        "                  inputs, labels = inputs.to(device), labels.to(device)\n",
        "                  outputs = model(inputs)\n",
        "                  _, preds = torch.max(outputs, 1)\n",
        "                  loss = criterion(outputs, labels)\n",
        "                    \n",
        "                  running_loss += loss.item()\n",
        "                  running_corrects += torch.sum(preds == labels.data)  \n",
        "\n",
        "                  print_val = f\"Steps {i} \\t\"\n",
        "                  print_val += f\"validation_running_loss : {(loss.item()):.6f}\\t\"\n",
        "                  print_val += f\"validation_running_corrects : {torch.sum(preds == labels.data)}\\t\"  \n",
        "                  print_val += f\"validation_total_corrects : {running_corrects}\\t\"  \n",
        "                  sys.stdout.write('\\r' + str(print_val))\n",
        "\n",
        "              epoch_val_loss = running_loss / len(testloaders)\n",
        "              epoch_val_acc = running_corrects.double().item() /len(testloaders.dataset)\n",
        "              all_validation_loss.append(epoch_val_loss)\n",
        "              all_validation_accuracy.append(epoch_val_acc)\n",
        "\n",
        "              print()\n",
        "              print(\"----------------------------Validation Summary-----------------\")\n",
        "              print('{} Validation Avg. Loss: {:.4f} Validation Avg. Acc: {:.4f}'.format(\n",
        "                    phase, epoch_val_loss, epoch_val_acc))\n",
        "              print(\"------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "          model.train()\n",
        "          \n",
        "          print(\"-------Start Model Save----\\n\\n\")\n",
        "          save_model(model_used,\n",
        "                     model_save_path,\n",
        "                     epoch, \n",
        "                     len(dataloaders),\n",
        "                     model,\n",
        "                     optimizer,\n",
        "                     IMAGE_SIZE,\n",
        "                     tranning_loss=all_tranning_loss,\n",
        "                     tranning_acc= all_tranning_accuracy,\n",
        "                     validation_loss=all_validation_loss,\n",
        "                     validation_acc=all_validation_accuracy,\n",
        "                     learning_rate=lernRate)\n",
        "          # break\n",
        "      \n",
        "    print(\"Complete Train\")\n",
        "          ## deep copy the model\n",
        "          # if phase == 'val' and epoch_acc > best_acc:\n",
        "          #       best_acc = epoch_acc\n",
        "          #       best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print()\n",
        "    \n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "    return "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWHT-avGu4di"
      },
      "source": [
        "## Start Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tafnrtV5jdi"
      },
      "source": [
        "# 1 (26 - 1 * 28 * 28) (own) (FMNIST)\n",
        "# 1_2 (300 - 1 * 28 * 28) (own) (FMNIST)\n",
        "# 1_3 (300 - 1 * 28 * 28) (own) (FMNIST)\n",
        "\n",
        "# 2 (100 - 1 * 128 * 128) (FMNIST) \n",
        "# 2 (100 - 1 * 128 * 128) (own) \n",
        "# 2 (100 - 3 * 128 * 128) (own) \n",
        "\n",
        "# 3 (25 - 1 * 128 * 128) (FMNIST) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veOjFgCygkhm"
      },
      "source": [
        "!mkdir 'EXP-3-COLOR-OWN'\n",
        "%cd '/content'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaKfUzXK0j2v"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "START=0\n",
        "model_save_path = './EXP-3-COLOR-OWN/' # must give `/` for the folder directory\n",
        "\n",
        "model_used= 'EXPERIMENT_MODEL_3_RSNET_COLOR'\n",
        "# model_ft = model_1\n",
        "# model_ft = model_1_2\n",
        "# model_ft = model_1_3\n",
        "# model_ft = model_2\n",
        "model_ft = model_3\n",
        "optimizer_ft = optimizer\n",
        "all_tranning_loss = [];\n",
        "all_validation_loss = [];\n",
        "all_tranning_accuracy = []; \n",
        "all_validation_accuracy = [];\n",
        "\n",
        "load_saved_model='/content/EXP-1-2-OWN/EXPERIMENT_MODEL_1_2-checkpoint-epoch-187.pt'\n",
        "p = Path(load_saved_model)\n",
        "if len(load_saved_model) > 1 and p.exists():\n",
        "  loadedModel = torch.load(load_saved_model, map_location=device)\n",
        "  model_used = loadedModel['MODEL_USED']\n",
        "  model_ft =  loadedModel['model_full']\n",
        "  model_ft.load_state_dict(loadedModel['model_state'])\n",
        "  optimizer_ft = loadedModel['optimizer_full']\n",
        "  optimizer_ft.load_state_dict(loadedModel['optimizer_state'] )\n",
        "  START = loadedModel['epoch'] + 1 \n",
        "  all_tranning_loss = loadedModel['tranning_loss'] \n",
        "  all_validation_loss = loadedModel['validation_loss'] \n",
        "  all_tranning_accuracy = loadedModel['tranning_acc'] \n",
        "  all_validation_accuracy = loadedModel['validation_acc']\n",
        "\n",
        "END=25\n",
        "trainloader= trainloader\n",
        "testloader = validationloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GBVkJFzuYRj"
      },
      "source": [
        "train_model(START, END,model_used, model_save_path, model_ft, criterion, optimizer_ft, trainloader, testloader, \n",
        "            0.01,\n",
        "            all_tranning_loss, \n",
        "            all_validation_loss, \n",
        "            all_tranning_accuracy, \n",
        "            all_validation_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuwKDsf70zJk"
      },
      "source": [
        "# !cp  '/content/drive/MyDrive/MY_COURSE/4.2/CSE-4238-Soft Computuing/Assignment 2/OWN/EXP-1-2-OWN.zip' './'\n",
        "# !unzip '/content/EXP-1-2-OWN.zip' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIwwFK_kDT2l"
      },
      "source": [
        "# !mkdir 'EXP-2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3VkXWLRJFHj"
      },
      "source": [
        "!zip -r 'EXP-3-COLOR-OWN.zip' 'EXP-3-COLOR-OWN' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3l0RHaGDcgN"
      },
      "source": [
        "# !mv '/content/EXPERIMENT_MODEL_2-checkpoint-epoch-26.pt' '/content/EXP-2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0xPnRY32xwf"
      },
      "source": [
        "# !rm -rf '/content/EXP-1/EXP-1/drive/MyDrive/MY_COURSE/4.2/CSE-4238-Soft Computuing/Assignment 2/EXP-1/drive'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf0H9JYOnlWX"
      },
      "source": [
        "!cp -rfv '/content/EXP-1-3-OWN.zip'  '/content/drive/Shareddrives/Test shared drive'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWIxDaG-E4Ro"
      },
      "source": [
        "!cp -rfv '/content/EXP-1-3-OWN.zip'  '/content/drive/MyDrive/MY_COURSE/4.2/CSE-4238-Soft Computuing/Assignment 2/OWN'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mce3NaDT_xR3"
      },
      "source": [
        "# class CNN(torch.nn.Module): \n",
        "#     def __init__(self):\n",
        "#         super(CNN, self).__init__()\n",
        "\n",
        "#         #initializing 4 convolution layer\n",
        "#         self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3)\n",
        "#         self.conv2 = torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n",
        "#         self.conv3 = torch.nn.Conv2d(in_channels=32, out_channels=128, kernel_size=3)\n",
        "#         self.conv4 = torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4)\n",
        "\n",
        "#         #initializing dropout \n",
        "#         self.dropout = torch.nn.Dropout(0.2)\n",
        "\n",
        "#         #initializing dropout \n",
        "#         self.pool= torch.nn.MaxPool2d(2,2)\n",
        " \n",
        "#         #initializing linear \n",
        "#         self.fc1 = torch.nn.Linear(256 * 5 * 5, 512)\n",
        "#         self.fc2 = torch.nn.Linear(512, 64)\n",
        "#         self.fc3 = torch.nn.Linear(64, 32)\n",
        "#         self.fc4 = torch.nn.Linear(32, 10)\n",
        " \n",
        "#     def forward(self, x):\n",
        "#         x = self.pool(torch.nn.functional.relu(self.conv1(x))) #sending input into 1st convolution layer,then to relu ,then to pooling layer , param = ((3*3*3)+1)*16 = 448\n",
        "#         x = self.pool(torch.nn.functional.relu(self.conv2(x))) #sending previous output into 2nd convolution layer,then to relu ,then to pooling layer, param = ((3*3*16)+1)*32 = 4640 \n",
        "#         x = self.dropout(x) #dropout unnecessary output\n",
        "#         x = self.pool(torch.nn.functional.relu(self.conv3(x)))\n",
        "#         x = self.pool(torch.nn.functional.relu(self.conv4(x)))\n",
        "#         x = self.dropout(x)\n",
        "#         x = x.view(-1, 256 * 5 * 5) # for flatten layer\n",
        "#         x = torch.nn.functional.relu(self.fc1(x))\n",
        "#         x = torch.nn.functional.relu(self.fc2(x))\n",
        "#         x = torch.nn.functional.relu(self.fc3(x))\n",
        "#         x = self.fc4(x)\n",
        "#         return x\n",
        "        \n",
        "# #select CPU or GPU as a device\n",
        "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# device\n",
        "# model = CNN().to(device)\n",
        "# print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFMzERf9EoFP"
      },
      "source": [
        "# Classification Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbWZXc4qFcEe"
      },
      "source": [
        "# import some importent library or packages \n",
        "import glob,sys,os\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import numpy as np\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "import zipfile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "import torch.optim as optim\n",
        "import time,sys\n",
        "import copy\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY3X4zCJE06P"
      },
      "source": [
        "## Download Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtzdDyOPEy8H"
      },
      "source": [
        "# !gdown --id 1bhegwtIpwAYCsnptXoYrSEmuEObyMq4Y ## insecptionV3 without as output\n",
        "# https://drive.google.com/file/d/1bwC7yVaHFsLmyiQFExewVkt33DeuhLpm/view?usp=sharing\n",
        "# https://drive.google.com/file/d/10jt7Oe7RrFtNWvsY1eQTRPS2nCNqmCnB/view?usp=sharing\n",
        "# https://drive.google.com/file/d/10s3Cln4xLViOizFxe1HHkSYKjq7_A_3X/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1GQpI5HSHF_cg8mvBsBxc89ohSa5fDqEv/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1-8AKgpmaqLDT5K6CGmfZ-SrgC5vtxqYG/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1VLPrGSLzQBbL_G1rTufcs7EmMZ_YJGH-/view?usp=sharing\n",
        "\n",
        "# https://drive.google.com/file/d/1IxM0bieCLAaWWe_r67sd_Jz-oS8qXVfN/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1-8x2xQQ0NOxpkF1MMe0k2Sz-PjnAS8Lc/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1LwOS_LP56Pm1xcbjXRzf_EYClpz167PK/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1qpsII4q2Xk1mdSrS3KW4K8Pe7ugY8rs3/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1-7a44W76z-joINkLC_0ffcdBkPkRY_y8/view?usp=sharing\n",
        "\n",
        "# https://drive.google.com/file/d/1PBTq5xRgadmgc_MocZIHGhCFY80QBAg2/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1-92tqY9DwsAHgUikt5B_9dMPc7Oq1sWH/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1VzWoWbjjMeptGSLgqqBxSdwiQ28CFmp9/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1K73sdUVBVx29VX8cBABH0AInev920sT0/view?usp=sharing\n",
        "!gdown --id 1K73sdUVBVx29VX8cBABH0AInev920sT0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8-5cwM59_sX"
      },
      "source": [
        "!rm -rf  '/content/MODEL'\n",
        "!mkdir '/content/MODEL'\n",
        "!unzip -u '/content/EXP-3-FMINST.zip' -d '/content/MODEL'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfKfNDX7E3cG"
      },
      "source": [
        "## Model Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZjk0Zk3-kp2"
      },
      "source": [
        "class LIN_MODEL(torch.nn.Module): \n",
        "    def __init__(self,outDim):\n",
        "        super(LIN_MODEL, self).__init__()\n",
        "\n",
        "        self.fc_1 = torch.nn.Linear(784, 90)\n",
        "        self.fc_2 = torch.nn.Linear(90, 50)\n",
        "        self.fc_3 = torch.nn.Linear(50, 30)\n",
        "        self.fc_4 = torch.nn.Linear(30, 18)\n",
        "        self.fc_5 = torch.nn.Linear(18, 12)\n",
        "        self.fc_6 = torch.nn.Linear(12, outDim)\n",
        "\n",
        "        self.linear = torch.nn.Linear(784, outDim)\n",
        " \n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(-1, 28 * 28) \n",
        "        x = torch.nn.functional.relu(self.fc_1(x))\n",
        "        x = torch.nn.functional.softmax(self.fc_2(x),dim=1)\n",
        "        x = torch.nn.functional.relu(self.fc_3(x))\n",
        "        x = torch.nn.functional.softmax(self.fc_4(x),dim=1)\n",
        "        x = torch.nn.functional.relu(self.fc_5(x))\n",
        "        x = self.fc_6(x)\n",
        "        # x = torch.nn.functional.softmax(self.linear(x),dim=1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LIN_MODEL_2(torch.nn.Module): \n",
        "    def __init__(self,outDim):\n",
        "        super(LIN_MODEL_2, self).__init__()\n",
        "\n",
        "        self.fc_1 = torch.nn.Linear(784, 742)\n",
        "        self.fc_2 = torch.nn.Linear(742, 621)\n",
        "        self.fc_3 = torch.nn.Linear(621, 510)\n",
        "        self.fc_6 = torch.nn.Linear(510, outDim)\n",
        " \n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(-1, 28 * 28) \n",
        "        x = torch.nn.functional.relu(self.fc_1(x))\n",
        "        x = torch.nn.functional.relu(self.fc_2(x))\n",
        "        x = torch.nn.functional.relu(self.fc_3(x))\n",
        "        x = torch.nn.functional.relu(self.fc_6(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class LIN_MODEL_3(torch.nn.Module): \n",
        "    def __init__(self,outDim):\n",
        "        super(LIN_MODEL_3, self).__init__()\n",
        "\n",
        "        self.fc_1 = torch.nn.Linear(784, 512)\n",
        "        self.fc_2 = torch.nn.Linear(512, 256)\n",
        "        self.fc_6 = torch.nn.Linear(256, outDim)\n",
        " \n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(-1, 28 * 28) \n",
        "        x = torch.nn.functional.relu(self.fc_1(x))\n",
        "        x = torch.nn.functional.relu(self.fc_2(x))\n",
        "        x = torch.nn.functional.relu(self.fc_6(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "class CNN(torch.nn.Module): \n",
        "    def __init__(self ,outDim):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        #initializing convolution layer\n",
        "        # self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3)\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3)\n",
        "        self.conv2 = torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n",
        "\n",
        "        #initializing dropout \n",
        "        self.dropout = torch.nn.Dropout(0.2)\n",
        "\n",
        "        #initializing dropout \n",
        "        self.pool= torch.nn.MaxPool2d(2,2)\n",
        " \n",
        "        #initializing linear\n",
        "        self.fc1 = torch.nn.Linear(32* 30* 30, 512)\n",
        "        self.fc2 = torch.nn.Linear(512,64)\n",
        "        self.fc3 = torch.nn.Linear(64,10)\n",
        "\n",
        " \n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.nn.functional.relu(self.conv1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.pool(torch.nn.functional.relu(self.conv2(x))) \n",
        "        x = self.dropout(x)\n",
        "        x = x.view(-1, 32* 30* 30) \n",
        "        # print(x.shape)\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        x = torch.nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m01AjAaUE4on"
      },
      "source": [
        "MODEL_LOAD_PATH = '/content/MODEL/EXP-3-FMINST/EXPERIMENT_MODEL_3-checkpoint-epoch-24.pt'\n",
        "model = torch.load(MODEL_LOAD_PATH,map_location='cpu')\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZz2b0suFmDm"
      },
      "source": [
        "## All the loss and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDQxaN7NFiQG"
      },
      "source": [
        "_tranning_loss = model['tranning_loss']\n",
        "_tranning_acc = model['tranning_acc']\n",
        "_validation_loss = model['validation_loss']\n",
        "_validation_acc = model['validation_acc']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RRZhE64Fi0-"
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"LOSS Exp-2 Model-3 FMINIST\")\n",
        "plt.plot(_tranning_loss,label=\"Tranning Loss\")\n",
        "plt.plot(_validation_loss,label=\"Validation Loss\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DMcblXtFqOp"
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Accuracy Exp-2 Model-3 FMINIST\")\n",
        "plt.plot(_tranning_acc,label=\"Tranning Accuracy\")\n",
        "plt.plot(_validation_acc,label=\"Validation Accuracy\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txd__RCzFn8P"
      },
      "source": [
        "## Load test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSTermOW_lum"
      },
      "source": [
        "\n",
        "IMAGE_SIZE = model['image_size'] ## set image height width to 128x128\n",
        "transform = transforms.Compose([\n",
        "                                # transforms.ToPILImage(),\n",
        "\n",
        "                                ## this is only for when model is 1\n",
        "                                transforms.Grayscale(), \n",
        "                                \n",
        "                                # transforms.RandomRotation(20,expand=True), ## adding random rotation 20deg\n",
        "                                # torchvision.transforms.ColorJitter(hue=.05, saturation=.05), ## adding color filter\n",
        "                                # transforms.RandomVerticalFlip(), ## adding vertical flip\n",
        "                                # transforms.RandomHorizontalFlip(), ## adding horizontal flip\n",
        "                                transforms.Resize(IMAGE_SIZE),  ## image resize\n",
        "                                transforms.CenterCrop(IMAGE_SIZE), ## image center crop\n",
        "                                transforms.ToTensor(), ## array converted into torch tensor and then divided by 255 (1.0/255)\n",
        "                                # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                               ])\n",
        "trainDataset = torchvision.datasets.FashionMNIST(root='./data', \n",
        "                            train=True, \n",
        "                            transform=transform,  # Normalize the image to [0-1] from [0-255]\n",
        "                            download=True)\n",
        "\n",
        "validDataSet = torchvision.datasets.FashionMNIST(root='./data', \n",
        "                           train=False, \n",
        "                           transform=transform)\n",
        "\n",
        "'''\n",
        "MAKING DATASET ITERABLE\n",
        "'''\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(dataset=trainDataset, \n",
        "                                           batch_size=1, \n",
        "                                           shuffle=True)   # It's better to shuffle the whole training dataset! \n",
        "\n",
        "testloader = torch.utils.data.DataLoader(dataset=validDataSet, \n",
        "                                          batch_size=1, \n",
        "                                          shuffle=False)  \n",
        "\n",
        "print(\"Length of the trainloader:\", len(trainloader ) * 1)\n",
        "print(\"Length of the validationloader:\", len(testloader ) * 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0oIxtF4FofN"
      },
      "source": [
        "# # BATCH_SIZE = 32 ## number of image batch size \n",
        "# base_dir = '/content/PROCESSED_DATASET-170104028' \n",
        "# TEST_SIZE = 0.2\n",
        "# IMAGE_SIZE = model['image_size'] ## set image height width to 128x128\n",
        "# transform = transforms.Compose([\n",
        "#                                 # transforms.ToPILImage(),\n",
        "\n",
        "#                                 ## this is only for when model is 1\n",
        "#                                 transforms.Grayscale(), \n",
        "                                \n",
        "#                                 # transforms.RandomRotation(20,expand=True), ## adding random rotation 20deg\n",
        "#                                 # torchvision.transforms.ColorJitter(hue=.05, saturation=.05), ## adding color filter\n",
        "#                                 # transforms.RandomVerticalFlip(), ## adding vertical flip\n",
        "#                                 # transforms.RandomHorizontalFlip(), ## adding horizontal flip\n",
        "#                                 transforms.Resize(IMAGE_SIZE),  ## image resize\n",
        "#                                 transforms.CenterCrop(IMAGE_SIZE), ## image center crop\n",
        "#                                 transforms.ToTensor(), ## array converted into torch tensor and then divided by 255 (1.0/255)\n",
        "#                                 # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "#                                ])\n",
        "\n",
        "# #load training dataset \n",
        "# dataset = torchvision.datasets.ImageFolder(base_dir, transform=transform)\n",
        "# n = len(dataset) \n",
        "# n_test = int(TEST_SIZE * n) # 10% validation\n",
        "# trainDataset, validDataSet = torch.utils.data.random_split(dataset,[n - n_test,n_test]) #random split dataset\n",
        "# trainloader = torch.utils.data.DataLoader(trainDataset, batch_size=1, shuffle=True, pin_memory=True,) \n",
        "# testloader = torch.utils.data.DataLoader(validDataSet, batch_size=1, shuffle=True, pin_memory=True,) \n",
        "# print(\"Length of the trainloader:\", len(trainloader ) * 1)\n",
        "# print(\"Length of the validationloader:\", len(testloader ) * 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5XTgUzpF7nA"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKoYidVPF9zI"
      },
      "source": [
        "## Load model with state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA0PI1auF9ZG"
      },
      "source": [
        "preTrainedModel = model['model_full']\n",
        "preTrainedModel.load_state_dict( model['model_state'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46IMMmn1GCOG"
      },
      "source": [
        "## Model prediction  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uomJnmc9GAgl"
      },
      "source": [
        "y_true_tensor = torch.tensor([])\n",
        "y_pred_tensor = torch.tensor([])\n",
        "preTrainedModel.eval()\n",
        "for i,(inputs, labels) in enumerate(testloader):\n",
        "\n",
        "  inputs, labels = inputs.to(device), labels.to(device)\n",
        "  outputs = preTrainedModel(inputs)\n",
        "  _, preds = torch.max(outputs, 1)\n",
        "  y_true_tensor = torch.cat((y_true_tensor,labels))\n",
        "  y_pred_tensor = torch.cat((y_pred_tensor,preds))\n",
        "  print(i)\n",
        "  # break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEUbIeiLGEwe"
      },
      "source": [
        "# Convert tensor to list\n",
        "y_true = y_true_tensor.tolist()\n",
        "y_pred = y_pred_tensor.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6zlRbKtGLXo"
      },
      "source": [
        "## Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z79lQHhGJ9N"
      },
      "source": [
        "matrice = confusion_matrix(y_true,y_pred)\n",
        "matrice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-G56ntEGMiu"
      },
      "source": [
        "\n",
        "## FOR F-MNIST\n",
        "target_names = list(validDataSet.class_to_idx.keys())\n",
        "df_cm = pd.DataFrame(matrice,columns=target_names,index=target_names)\n",
        "plt.figure(figsize=(15,15))\n",
        "sn.heatmap(df_cm, annot=True,annot_kws={\"size\": 10},fmt='g',cmap='Blues',)\n",
        "plt.title(f\"Confusion matrix {model['MODEL_USED']}\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "## FOR OWN\n",
        "# target_names = list(dataset.class_to_idx.keys())\n",
        "# df_cm = pd.DataFrame(matrice,columns=target_names,index=target_names)\n",
        "# plt.figure(figsize=(15,15))\n",
        "# sn.heatmap(df_cm, annot=True,annot_kws={\"size\": 10},fmt='g',cmap='Blues',)\n",
        "# plt.title(f\"Confusion matrix {model['MODEL_USED']}\")\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CaphhnQGOBm"
      },
      "source": [
        "## Classification Report `recision,  recall,  f1-score`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNVWNb-jGQJF"
      },
      "source": [
        "## FOR FNIMST\n",
        "target_names = list(validDataSet.class_to_idx.keys())\n",
        "classify_report = classification_report(y_true, y_pred, target_names=target_names)\n",
        "print(classify_report)\n",
        "\n",
        "\n",
        "## FOR OWN\n",
        "# target_names = list(dataset.class_to_idx.keys())\n",
        "# classify_report = classification_report(y_true, y_pred, target_names=target_names)\n",
        "# print(classify_report)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}