{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSE 4238 - Soft Computing Lab - Assignment 2- All in One.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "LVJMNAzd6cyj",
        "yVEH4KHhjoC9",
        "and1qRpqjrrG",
        "K1W3UGlL5h96",
        "I6btXeeG5cLi",
        "D5AWPysr52J6",
        "Qzg5GVjV555R",
        "nH_5Fjucw7Vs",
        "v1Q9jiYY6SwS",
        "CZokUlmS-QlQ",
        "Muzux96C5B4p",
        "ITcWE8G--CQq",
        "vu-fq2VRtEWs",
        "WUibX4ou1S6g",
        "3OMDfjb93LSi",
        "u77XBDPsK_Y7",
        "PH-sgSiwfVjc",
        "z4ZOi5mGuLcD",
        "0w0oLNMdu0Jr",
        "bWHT-avGu4di",
        "tFMzERf9EoFP"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rafat97/bengali-handwritten-digits-0-to-9-classification/blob/master/CSE_4238_Soft_Computing_Lab_Assignment_2_All_in_One.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EYVTkeU6hmy"
      },
      "source": [
        "# Notebook Author info\n",
        "<center>\n",
        "\n",
        "| |  |\n",
        "| ----------- | ----------- |\n",
        "| ![Emdadul Haque Rafat](https://rafat97.github.io/static/c3688eb99d1fef50023a121e3abc5fa6/e8044/my-image.jpg)      | `Name:` Emdadul Haque<br /><br /> `Professional Status:` Student of Computer Science and Engineering <br /><br /> `Email:` rafathaque1997@gmail.com <br /><br /> `Website :` https://rafat97.github.io/ <br /><br />`Github:` https://github.com/Rafat97 <br /><br /> `Linkedin:` https://www.linkedin.com/in/rafat-haque-173131139/   |\n",
        "\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVJMNAzd6cyj"
      },
      "source": [
        "# Drive mount code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP2_K3IB3rHL"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVEH4KHhjoC9"
      },
      "source": [
        "# Dataset Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXSLNONRQYW4"
      },
      "source": [
        "# import some importent library or packages \n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import time,sys\n",
        "import copy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import pathlib\n",
        "import zipfile\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "import torch.optim as optim\n",
        "import pathlib\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "!pip install torchviz\n",
        "from torchviz import make_dot, make_dot_from_trace\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpcmFapg4GQC"
      },
      "source": [
        "!gdown --id 1txyKhs1Zt5AKswGGK9VI_jE0JNHuQT85"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gug3bvwK4IpM"
      },
      "source": [
        "!unzip '/content/Dataset A.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "and1qRpqjrrG"
      },
      "source": [
        "## Read `csv` file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXmL0p_TrEfO"
      },
      "source": [
        "!rm -rf '/content/PROCESSED_DATASET-170104028'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14wgV0kKi6zP"
      },
      "source": [
        "traning_csv = '/content/training-a.csv'\n",
        "read_df = pd.read_csv(traning_csv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imUxJA6lmdj2"
      },
      "source": [
        "read_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZiqjtWFykiz"
      },
      "source": [
        "number_of_digit_class = 10\n",
        "for i in range(0,number_of_digit_class):\n",
        "  select_digit = read_df[read_df['digit'] == i]\n",
        "  print(i , select_digit.shape )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1W3UGlL5h96"
      },
      "source": [
        "## Dataset processed & store into `digit` based folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyUwXfEkjQoP"
      },
      "source": [
        "number_of_digit_class = read_df['digit'].nunique()\n",
        "\n",
        "DATASET_ROOT_DIR = './PROCESSED_DATASET_170104028/' \n",
        "path = Path(DATASET_ROOT_DIR)\n",
        "path.mkdir(parents=True, exist_ok=True)\n",
        "DATASET_ROOT_DIR = os.path.abspath(path)\n",
        "\n",
        "for i in range(0,number_of_digit_class):\n",
        "  select_digit = read_df[read_df['digit'] == i]\n",
        "  for index,val in select_digit.iterrows():\n",
        "    file_relative_path_from= f\"./{val['database name']}/{val['filename']}\"\n",
        "    file_relative_dir_to = f\"{DATASET_ROOT_DIR}/{val['digit']}\"\n",
        "    Path(file_relative_dir_to).mkdir(parents=True, exist_ok=True)\n",
        "    shutil.copy(file_relative_path_from, file_relative_dir_to)\n",
        "    print(file_relative_path_from , file_relative_dir_to , \"OK\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6btXeeG5cLi"
      },
      "source": [
        "## Check Image is correct"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO3fbVc9xIrv"
      },
      "source": [
        "#@title #<center>All images are correct or not using pytorch `ImageFolder`</center> { display-mode: \"form\" }\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "        # transforms.RandomResizedCrop(256),\n",
        "        # transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "      ])\n",
        "\n",
        "img_path = '/content/PROCESSED_DATASET-170104028'  #@param {type: \"string\"}\n",
        "train_dataset=datasets.ImageFolder(root=img_path,transform=train_transforms)\n",
        "print( train_dataset.class_to_idx )\n",
        "dataloaders = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=False,pin_memory=True)\n",
        "for image,label in dataloaders:\n",
        "\n",
        "  plt.figure(figsize=(20,20))\n",
        "  grid_imge_gen = torchvision.utils.make_grid(image)\n",
        "  plt.imshow(grid_imge_gen.permute(1, 2, 0).cpu())\n",
        "  plt.show()\n",
        "  print(label)\n",
        "  # break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5AWPysr52J6"
      },
      "source": [
        "## Create dataset zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziguq0Fxywtb"
      },
      "source": [
        "ZIP_FILE_NAME = 'PROCESSED_DATASET-170104028.zip'\n",
        "!zip -r $ZIP_FILE_NAME 'PROCESSED_DATASET-170104028/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qzg5GVjV555R"
      },
      "source": [
        "## Copy the zip file into drive or safe place "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A801vVA3hF0"
      },
      "source": [
        "!cp '/content/PROCESSED_DATASET-170104028.zip'  '/content/drive/MyDrive/Datasets'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH_5Fjucw7Vs"
      },
      "source": [
        "# Data Load From zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlUs8I-8bvII",
        "outputId": "56d24b11-3a75-4ca9-dd5d-4d7c3446f837"
      },
      "source": [
        "# import some importent library or packages \n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import time,sys\n",
        "import copy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import pathlib\n",
        "import zipfile\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "import torch.optim as optim\n",
        "import pathlib\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "!pip install torchviz\n",
        "from torchviz import make_dot, make_dot_from_trace\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "torch.manual_seed(0)\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n",
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.7/dist-packages (0.0.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (3.7.4.3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fa3a0398cb0>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaKNziom4vdo"
      },
      "source": [
        "## Download preprocessed dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyR7nygy4chy",
        "outputId": "105e54a6-f568-44ef-dbe1-224adae57e9a"
      },
      "source": [
        "!gdown --id '1cml-H4UUJyY0hRoVeAynO8JjvWqa7FU1'"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1cml-H4UUJyY0hRoVeAynO8JjvWqa7FU1\n",
            "To: /content/PROCESSED_DATASET-170104028.zip\n",
            "816MB [00:14, 54.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMKBFffe458J"
      },
      "source": [
        "## Unzip the download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4KGVLdE4qXK",
        "outputId": "26813076-f248-4a83-bcc6-1073ec5da5a3"
      },
      "source": [
        "!unzip '/content/PROCESSED_DATASET-170104028.zip'"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/PROCESSED_DATASET-170104028.zip\n",
            "replace PROCESSED_DATASET-170104028/5/a14515.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQQQa_d35TTZ"
      },
      "source": [
        "## Remove the zip file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyZ6yOMA5PQJ"
      },
      "source": [
        "!rm '/content/PROCESSED_DATASET-170104028.zip'"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1Q9jiYY6SwS"
      },
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMbUWiw3Nj9J",
        "outputId": "6619a473-c2ec-4b13-c874-4b78241cef3c"
      },
      "source": [
        "# import some importent library or packages \n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import time,sys\n",
        "import copy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import pathlib\n",
        "import zipfile\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "import torch.optim as optim\n",
        "import pathlib\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "!pip install torchviz\n",
        "from torchviz import make_dot, make_dot_from_trace\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "torch.manual_seed(0)\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n",
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.7/dist-packages (0.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.9.0+cu102)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (3.7.4.3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fa3a0398cb0>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZokUlmS-QlQ"
      },
      "source": [
        "# important variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVsnfKIp-P-Q"
      },
      "source": [
        "base_dir = '/content/PROCESSED_DATASET-170104028' \n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# 28(model-1), 128(model-2), 224(model-3),\n",
        "IMAGE_SIZE = 224\n",
        "LEARNING_RATE = 0.01\n",
        "TEST_SIZE = 0.2\n",
        "OUTPUT_DIM=10"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Muzux96C5B4p"
      },
      "source": [
        "# Load Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLPbZAzwN0Vs",
        "outputId": "019cf1eb-9db6-44d2-cfbb-6928150e1d07"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "                                # transforms.ToPILImage(),\n",
        "\n",
        "                                ## this is only for when model is 1\n",
        "                                # transforms.Grayscale(), \n",
        "                                \n",
        "                                # transforms.RandomRotation(20,expand=True), ## adding random rotation 20deg\n",
        "                                # torchvision.transforms.ColorJitter(hue=.05, saturation=.05), ## adding color filter\n",
        "                                # transforms.RandomVerticalFlip(), ## adding vertical flip\n",
        "                                # transforms.RandomHorizontalFlip(), ## adding horizontal flip\n",
        "                                transforms.Resize(IMAGE_SIZE),  ## image resize\n",
        "                                transforms.CenterCrop(IMAGE_SIZE), ## image center crop\n",
        "                                transforms.ToTensor(), ## array converted into torch tensor and then divided by 255 (1.0/255)\n",
        "                                # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                               ])\n",
        "\n",
        "#load training dataset \n",
        "dataset = torchvision.datasets.ImageFolder(base_dir, transform=transform)\n",
        "n = len(dataset) \n",
        "n_test = int(TEST_SIZE * n) # 10% validation\n",
        "trainDataset, validDataSet = torch.utils.data.random_split(dataset,[n - n_test,n_test]) #random split dataset\n",
        "trainloader = torch.utils.data.DataLoader(trainDataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,) \n",
        "validationloader = torch.utils.data.DataLoader(validDataSet, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,) \n",
        "print(\"Length of the trainloader:\", len(trainloader ) * BATCH_SIZE)\n",
        "print(\"Length of the validationloader:\", len(validationloader ) * BATCH_SIZE)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the trainloader: 15776\n",
            "Length of the validationloader: 3968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Svy95MXy_5BB"
      },
      "source": [
        "\n",
        "# trainDataset = torchvision.datasets.FashionMNIST(root='./data', \n",
        "#                             train=True, \n",
        "#                             transform=transforms.ToTensor(),  # Normalize the image to [0-1] from [0-255]\n",
        "#                             download=True)\n",
        "\n",
        "# validDataSet = torchvision.datasets.FashionMNIST(root='./data', \n",
        "#                            train=False, \n",
        "#                            transform=transforms.ToTensor())\n",
        "\n",
        "# '''\n",
        "# MAKING DATASET ITERABLE\n",
        "# '''\n",
        "\n",
        "# trainloader = torch.utils.data.DataLoader(dataset=trainDataset, \n",
        "#                                            batch_size=BATCH_SIZE, \n",
        "#                                            shuffle=True)   # It's better to shuffle the whole training dataset! \n",
        "\n",
        "# validationloader = torch.utils.data.DataLoader(dataset=validDataSet, \n",
        "#                                           batch_size=BATCH_SIZE, \n",
        "#                                           shuffle=False)  \n",
        "\n",
        "# print(\"Length of the trainloader:\", len(trainloader ) * BATCH_SIZE)\n",
        "# print(\"Length of the validationloader:\", len(validationloader ) * BATCH_SIZE)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITcWE8G--CQq"
      },
      "source": [
        "# Data basic info."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah_Uya58td5d"
      },
      "source": [
        "dataset.class_to_idx # class map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMKQzdDG7NZ5"
      },
      "source": [
        "print(dict(Counter(dataset.targets))) # count number of data in a class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM9GI3D17tUp"
      },
      "source": [
        "train_classes = [label for _, label in trainDataset]\n",
        "print(dict(Counter(train_classes)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4u7UWJl858x"
      },
      "source": [
        "val_classes = [label for _, label in validDataSet]\n",
        "print(dict(Counter(val_classes)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8e6dxJq-dwY"
      },
      "source": [
        "for image,label in trainloader:\n",
        "  plt.figure(figsize=(20,20))\n",
        "  grid_imge_gen = torchvision.utils.make_grid(image)\n",
        "  plt.imshow(grid_imge_gen.permute(1, 2, 0).cpu())\n",
        "  plt.title(\"Trainloader\")\n",
        "  plt.show()\n",
        "  print(label)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAnMIFYL-opY"
      },
      "source": [
        "for image,label in validationloader:\n",
        "  plt.figure(figsize=(20,20))\n",
        "  grid_imge_gen = torchvision.utils.make_grid(image)\n",
        "  plt.imshow(grid_imge_gen.permute(1, 2, 0).cpu())\n",
        "  plt.title(\"Validationloader\")\n",
        "  plt.show()\n",
        "  print(label)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpJj9oCT_wEx"
      },
      "source": [
        "# Model Creation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-tEKJ2uV8K0"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu-fq2VRtEWs"
      },
      "source": [
        "## Model 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13hvC1cTV6AN"
      },
      "source": [
        "'''\n",
        "Model creation \n",
        "'''\n",
        "\n",
        "class LIN_MODEL(torch.nn.Module): \n",
        "    def __init__(self,outDim):\n",
        "        super(LIN_MODEL, self).__init__()\n",
        "\n",
        "        self.fc_1 = torch.nn.Linear(784, 90)\n",
        "        self.fc_2 = torch.nn.Linear(90, 50)\n",
        "        self.fc_3 = torch.nn.Linear(50, 30)\n",
        "        self.fc_4 = torch.nn.Linear(30, 18)\n",
        "        self.fc_5 = torch.nn.Linear(18, 12)\n",
        "        self.fc_6 = torch.nn.Linear(12, outDim)\n",
        "\n",
        "        self.linear = torch.nn.Linear(784, outDim)\n",
        " \n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(-1, 28 * 28) \n",
        "        x = torch.nn.functional.relu(self.fc_1(x))\n",
        "        x = torch.nn.functional.softmax(self.fc_2(x),dim=1)\n",
        "        x = torch.nn.functional.relu(self.fc_3(x))\n",
        "        x = torch.nn.functional.softmax(self.fc_4(x),dim=1)\n",
        "        x = torch.nn.functional.relu(self.fc_5(x))\n",
        "        x = self.fc_6(x)\n",
        "        # x = torch.nn.functional.softmax(self.linear(x),dim=1)\n",
        "        return x\n",
        "        \n",
        "\n",
        "model_1 = LIN_MODEL(OUTPUT_DIM).to(device)\n",
        "\n",
        "summary( model_1, input_size=(1, 28, 28))\n",
        "# select CPU or GPU as a device\n",
        "print(model_1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykGaF2ILrB-H"
      },
      "source": [
        "x = torch.randn(BATCH_SIZE,1,28,28).to(device)\n",
        "make_dot(model_1(x), params=dict(model_1.named_parameters()), show_attrs=True, show_saved=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d65L4VNBmWY"
      },
      "source": [
        "make_dot(model_1(x), params=dict(model_1.named_parameters()), show_attrs=True, show_saved=True).render(\"Model-1\", format=\"png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lKpdU7gtOcS"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_1.parameters(),lr=LEARNING_RATE, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUibX4ou1S6g"
      },
      "source": [
        "## Model 1 v2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX3e6Zyc1WkH"
      },
      "source": [
        "class LIN_MODEL_2(torch.nn.Module): \n",
        "    def __init__(self,outDim):\n",
        "        super(LIN_MODEL_2, self).__init__()\n",
        "\n",
        "        self.fc_1 = torch.nn.Linear(784, 742)\n",
        "        self.fc_2 = torch.nn.Linear(742, 621)\n",
        "        self.fc_3 = torch.nn.Linear(621, 510)\n",
        "        self.fc_6 = torch.nn.Linear(510, outDim)\n",
        " \n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(-1, 28 * 28) \n",
        "        x = torch.nn.functional.relu(self.fc_1(x))\n",
        "        x = torch.nn.functional.relu(self.fc_2(x))\n",
        "        x = torch.nn.functional.relu(self.fc_3(x))\n",
        "        x = torch.nn.functional.relu(self.fc_6(x))\n",
        "\n",
        "        return x\n",
        "        \n",
        "model_1_2 = LIN_MODEL_2(OUTPUT_DIM).to(device)\n",
        "\n",
        "summary( model_1_2, input_size=(1, 28, 28))\n",
        "# select CPU or GPU as a device\n",
        "print(model_1_2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAN4CWc21mED"
      },
      "source": [
        "x = torch.randn(BATCH_SIZE,1,28,28).to(device)\n",
        "make_dot(model_1_2(x), params=dict(model_1_2.named_parameters()), show_attrs=True, show_saved=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3FOVAipBilb"
      },
      "source": [
        "make_dot(model_1_2(x), params=dict(model_1_2.named_parameters()), show_attrs=True, show_saved=True).render(\"Model-1-2\", format=\"png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoTH_ZBv1skg"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_1_2.parameters(),lr=LEARNING_RATE, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OMDfjb93LSi"
      },
      "source": [
        "## Model 1 V3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsaXBHQ83K1m"
      },
      "source": [
        "class LIN_MODEL_3(torch.nn.Module): \n",
        "    def __init__(self,outDim):\n",
        "        super(LIN_MODEL_3, self).__init__()\n",
        "\n",
        "        self.fc_1 = torch.nn.Linear(784, 512)\n",
        "        self.fc_2 = torch.nn.Linear(512, 256)\n",
        "        self.fc_6 = torch.nn.Linear(256, outDim)\n",
        " \n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(-1, 28 * 28) \n",
        "        x = torch.nn.functional.relu(self.fc_1(x))\n",
        "        x = torch.nn.functional.relu(self.fc_2(x))\n",
        "        x = torch.nn.functional.relu(self.fc_6(x))\n",
        "\n",
        "        return x\n",
        "        \n",
        "model_1_3 = LIN_MODEL_3(OUTPUT_DIM).to(device)\n",
        "\n",
        "summary( model_1_3, input_size=(1, 28, 28))\n",
        "print(model_1_3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYyk1_LZ3cNP"
      },
      "source": [
        "x = torch.randn(BATCH_SIZE,1,28,28).to(device)\n",
        "make_dot(model_1_3(x), params=dict(model_1_3.named_parameters()), show_attrs=True, show_saved=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRV0Yc-X3hmS"
      },
      "source": [
        "make_dot(model_1_3(x), params=dict(model_1_3.named_parameters()), show_attrs=True, show_saved=True).render(\"Model-1-3\", format=\"png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAcK1jAI3nwi"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_1_3.parameters(),lr=LEARNING_RATE, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u77XBDPsK_Y7"
      },
      "source": [
        "## Model 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BNNndDmK_I5"
      },
      "source": [
        "class CNN(torch.nn.Module): \n",
        "    def __init__(self ,outDim):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        #initializing convolution layer\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3)\n",
        "        self.conv2 = torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n",
        "\n",
        "        #initializing dropout \n",
        "        self.dropout = torch.nn.Dropout(0.2)\n",
        "\n",
        "        #initializing dropout \n",
        "        self.pool= torch.nn.MaxPool2d(2,2)\n",
        " \n",
        "        #initializing linear\n",
        "        self.fc1 = torch.nn.Linear(32* 30* 30, 512)\n",
        "        self.fc2 = torch.nn.Linear(512,64)\n",
        "        self.fc3 = torch.nn.Linear(64,10)\n",
        "\n",
        " \n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.nn.functional.relu(self.conv1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.pool(torch.nn.functional.relu(self.conv2(x))) \n",
        "        x = self.dropout(x)\n",
        "        x = x.view(-1, 32* 30* 30) \n",
        "        # print(x.shape)\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        x = torch.nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "        \n",
        "\n",
        "model_2 = CNN(OUTPUT_DIM).to(device)\n",
        "\n",
        "summary( model_2, input_size=(3, 128, 128))\n",
        "# select CPU or GPU as a device\n",
        "print(model_2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LXmRQkmLX1E"
      },
      "source": [
        "x = torch.randn(1,3,128,128).to(device)\n",
        "make_dot(model_2(x), params=dict(model_2.named_parameters()), show_attrs=True, show_saved=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inF2IqPsahxB"
      },
      "source": [
        "make_dot(model_2(x), params=dict(model_2.named_parameters()), show_attrs=True, show_saved=True).render(\"Model-2-CNN\", format=\"png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVOSizzEyt04"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_2.parameters(),lr=LEARNING_RATE, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH-sgSiwfVjc"
      },
      "source": [
        "## Model 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q23zLm-RRir9"
      },
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "      \n",
        "num_classes = OUTPUT_DIM\n",
        "\n",
        "\n",
        "model_3 = models.resnet152(pretrained=True)\n",
        "set_parameter_requires_grad(model_3, True)\n",
        "num_ftrs = model_3.fc.in_features\n",
        "model_3.fc = torch.nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "model_3 = model_3.to(device)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "i9dSDf63ao2p",
        "outputId": "d8bc83f4-161d-421f-a55e-783f1016a640"
      },
      "source": [
        "x = torch.randn(1,3,224,224).to(device)\n",
        "make_dot(model_3(x), params=dict(model_3.named_parameters()), show_attrs=True, show_saved=True)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7fa399c2ad10>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"213pt\" height=\"380pt\"\n viewBox=\"0.00 0.00 213.00 380.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 376)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-376 209,-376 209,4 -4,4\"/>\n<!-- 140340637006336 -->\n<g id=\"node1\" class=\"node\">\n<title>140340637006336</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"91,-31 26,-31 26,0 91,0 91,-31\"/>\n<text text-anchor=\"middle\" x=\"58.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (1, 10)</text>\n</g>\n<!-- 140340636067088 -->\n<g id=\"node2\" class=\"node\">\n<title>140340636067088</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"195,-185 10,-185 10,-67 195,-67 195,-185\"/>\n<text text-anchor=\"middle\" x=\"102.5\" y=\"-173\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward</text>\n<text text-anchor=\"middle\" x=\"102.5\" y=\"-162\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;</text>\n<text text-anchor=\"middle\" x=\"102.5\" y=\"-151\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">alpha &#160;&#160;&#160;&#160;&#160;&#160;: &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;1</text>\n<text text-anchor=\"middle\" x=\"102.5\" y=\"-140\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">beta &#160;&#160;&#160;&#160;&#160;&#160;&#160;: &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;1</text>\n<text text-anchor=\"middle\" x=\"102.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">mat1 &#160;&#160;&#160;&#160;&#160;&#160;&#160;: [saved tensor]</text>\n<text text-anchor=\"middle\" x=\"102.5\" y=\"-118\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">mat1_sizes &#160;: &#160;&#160;&#160;&#160;&#160;(1, 2048)</text>\n<text text-anchor=\"middle\" x=\"102.5\" y=\"-107\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">mat1_strides: &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;()</text>\n<text text-anchor=\"middle\" x=\"102.5\" y=\"-96\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">mat2 &#160;&#160;&#160;&#160;&#160;&#160;&#160;: &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;None</text>\n<text text-anchor=\"middle\" x=\"102.5\" y=\"-85\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">mat2_sizes &#160;: &#160;&#160;&#160;&#160;(2048, 10)</text>\n<text text-anchor=\"middle\" x=\"102.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">mat2_strides: &#160;&#160;&#160;&#160;&#160;(1, 2048)</text>\n</g>\n<!-- 140340636067088&#45;&gt;140340637006336 -->\n<g id=\"edge7\" class=\"edge\">\n<title>140340636067088&#45;&gt;140340637006336</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M78.9543,-66.8681C75.2764,-57.6318 71.6565,-48.5408 68.5084,-40.6346\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"71.7553,-39.3277 64.8041,-31.332 65.2519,-41.9173 71.7553,-39.3277\"/>\n</g>\n<!-- 140340636740016 -->\n<g id=\"node3\" class=\"node\">\n<title>140340636740016</title>\n<polygon fill=\"#ffa500\" stroke=\"#000000\" points=\"186,-30.5 109,-30.5 109,-.5 186,-.5 186,-30.5\"/>\n<text text-anchor=\"middle\" x=\"147.5\" y=\"-18.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">mat1</text>\n<text text-anchor=\"middle\" x=\"147.5\" y=\"-7.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (1, 2048)</text>\n</g>\n<!-- 140340636067088&#45;&gt;140340636740016 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140340636067088&#45;&gt;140340636740016</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M126.5809,-66.8681C132.014,-53.5267 137.3236,-40.4887 141.2341,-30.8863\"/>\n</g>\n<!-- 140340636067664 -->\n<g id=\"node4\" class=\"node\">\n<title>140340636067664</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"101,-240 0,-240 0,-221 101,-221 101,-240\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-228\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140340636067664&#45;&gt;140340636067088 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140340636067664&#45;&gt;140340636067088</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M55.2988,-220.8562C58.5658,-214.2908 63.2146,-204.9485 68.3335,-194.6615\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"71.6084,-195.9365 72.93,-185.4244 65.3414,-192.8179 71.6084,-195.9365\"/>\n</g>\n<!-- 140340638147680 -->\n<g id=\"node5\" class=\"node\">\n<title>140340638147680</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"80,-306 21,-306 21,-276 80,-276 80,-306\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">fc.bias</text>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-283\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (10)</text>\n</g>\n<!-- 140340638147680&#45;&gt;140340636067664 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140340638147680&#45;&gt;140340636067664</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-275.7333C50.5,-268.0322 50.5,-258.5977 50.5,-250.3414\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-250.0864 50.5,-240.0864 47.0001,-250.0864 54.0001,-250.0864\"/>\n</g>\n<!-- 140340636068624 -->\n<g id=\"node6\" class=\"node\">\n<title>140340636068624</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"190,-240 119,-240 119,-221 190,-221 190,-240\"/>\n<text text-anchor=\"middle\" x=\"154.5\" y=\"-228\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140340636068624&#45;&gt;140340636067088 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140340636068624&#45;&gt;140340636067088</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M149.7012,-220.8562C146.4342,-214.2908 141.7854,-204.9485 136.6665,-194.6615\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"139.6586,-192.8179 132.07,-185.4244 133.3916,-195.9365 139.6586,-192.8179\"/>\n</g>\n<!-- 140340636066960 -->\n<g id=\"node7\" class=\"node\">\n<title>140340636066960</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"205,-300.5 104,-300.5 104,-281.5 205,-281.5 205,-300.5\"/>\n<text text-anchor=\"middle\" x=\"154.5\" y=\"-288.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140340636066960&#45;&gt;140340636068624 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140340636066960&#45;&gt;140340636068624</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M154.5,-281.2796C154.5,-273.0376 154.5,-260.9457 154.5,-250.629\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"158.0001,-250.3972 154.5,-240.3972 151.0001,-250.3973 158.0001,-250.3972\"/>\n</g>\n<!-- 140340636554416 -->\n<g id=\"node8\" class=\"node\">\n<title>140340636554416</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"196,-372 113,-372 113,-342 196,-342 196,-372\"/>\n<text text-anchor=\"middle\" x=\"154.5\" y=\"-360\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">fc.weight</text>\n<text text-anchor=\"middle\" x=\"154.5\" y=\"-349\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (10, 2048)</text>\n</g>\n<!-- 140340636554416&#45;&gt;140340636066960 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140340636554416&#45;&gt;140340636066960</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M154.5,-341.6924C154.5,-332.5067 154.5,-320.7245 154.5,-310.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"158.0001,-310.703 154.5,-300.7031 151.0001,-310.7031 158.0001,-310.703\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "h25Fz-2dapNZ",
        "outputId": "6f74654c-2ec4-4684-9fda-2d2bf84adb5b"
      },
      "source": [
        "make_dot(model_3(x), params=dict(model_3.named_parameters()), show_attrs=True, show_saved=True).render(\"Model-3-RSNET-152\", format=\"png\")"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Model-3-RSNET-152.png'"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBIOrzUaMqgB"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_3.parameters(),lr=LEARNING_RATE, )"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dbj7OlK0EKb9"
      },
      "source": [
        "# Start Tranning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4ZOi5mGuLcD"
      },
      "source": [
        "## Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py448GSDuJTL"
      },
      "source": [
        "def save_model(\n",
        "               MODEL_USED,\n",
        "               SAVEPATH,\n",
        "               epoch, \n",
        "               batch_size, \n",
        "               model,\n",
        "               optimizer,\n",
        "               image_size,\n",
        "               tranning_loss=[],\n",
        "               tranning_acc=[],\n",
        "               validation_loss=[],\n",
        "               validation_acc=[],\n",
        "               learning_rate=0.001,\n",
        "               meta_data=None):\n",
        "  SAVEPATH += f\"{MODEL_USED}-checkpoint-epoch-{epoch}.pt\"\n",
        "  save_obj = {\n",
        "       'MODEL_USED':MODEL_USED,\n",
        "       'batch_size':batch_size,\n",
        "       'epoch': epoch,\n",
        "       'model_full': model,\n",
        "       'optimizer_full': optimizer,\n",
        "       'model_state': model.state_dict(),\n",
        "       'optimizer_state': optimizer.state_dict(),\n",
        "       'image_size': image_size,\n",
        "       'tranning_loss': tranning_loss,\n",
        "       'tranning_acc': tranning_acc,\n",
        "       'validation_loss': validation_loss,\n",
        "       'validation_acc': validation_acc,\n",
        "       'learning_rate':learning_rate,\n",
        "       'meta_data':meta_data\n",
        "       }\n",
        "\n",
        "  torch.save(save_obj, SAVEPATH)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w0oLNMdu0Jr"
      },
      "source": [
        "## Train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjWXGOHiuTnr"
      },
      "source": [
        "def train_model(start, end, \n",
        "                model_used ,\n",
        "                model_save_path, \n",
        "                model, \n",
        "                criterion, \n",
        "                optimizer, \n",
        "                dataloaders,\n",
        "                testloaders , \n",
        "                lernRate=0.001,\n",
        "                all_tranning_loss=[], all_validation_loss=[], all_tranning_accuracy=[], all_validation_accuracy=[]):\n",
        "    since = time.time()\n",
        "    num_epochs = end\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    phase = 'train'\n",
        "    steps = 0\n",
        "\n",
        "    for epoch in range(start,num_epochs):\n",
        "          model.train()\n",
        "          phase = 'tranning'\n",
        "          print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "          print('-' * 10)\n",
        "          running_loss = 0.0\n",
        "          running_corrects = 0\n",
        "\n",
        "          for i,(inputs, labels) in enumerate(dataloaders):\n",
        "                \n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # running_loss += loss.item() * inputs.size(0)\n",
        "                running_loss += loss.item()\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                \n",
        "\n",
        "                print_val = f\"Epoch: {epoch}/{num_epochs-1} Steps {steps} \\t\"\n",
        "                print_val += f\"running_loss : {(loss.item()):.6f}\\t\"\n",
        "                print_val += f\"running_corrects : {torch.sum(preds == labels.data)}\\t\"  \n",
        "                print_val += f\"total_corrects : {running_corrects}\\t\"  \n",
        "                sys.stdout.write('\\r' + str(print_val))\n",
        "                steps += 1\n",
        "          \n",
        "          \n",
        "          steps = 0\n",
        "          epoch_loss = running_loss / len(dataloaders)\n",
        "          epoch_acc = running_corrects.double().item() /len(dataloaders.dataset)\n",
        "          all_tranning_loss.append(loss.item())\n",
        "          all_tranning_accuracy.append(epoch_acc)\n",
        "          \n",
        "          \n",
        "          print(\"\\n\")\n",
        "          print(\"----------------------------Tranning Summary----------------------\")\n",
        "          print('{} Tranning Avg. Loss: {:.4f} Tranning Avg. Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "          print(\"-\"*70)\n",
        "          print(\"\\n\")\n",
        "\n",
        "          print(\"Start Validation\")\n",
        "          model.eval()\n",
        "          with torch.no_grad():\n",
        "              phase = \"validation\"\n",
        "              running_loss = 0.0\n",
        "              running_corrects = 0\n",
        "              for i,(inputs, labels) in enumerate(testloaders):\n",
        "                  inputs, labels = inputs.to(device), labels.to(device)\n",
        "                  outputs = model(inputs)\n",
        "                  _, preds = torch.max(outputs, 1)\n",
        "                  loss = criterion(outputs, labels)\n",
        "                    \n",
        "                  running_loss += loss.item()\n",
        "                  running_corrects += torch.sum(preds == labels.data)  \n",
        "\n",
        "                  print_val = f\"Steps {i} \\t\"\n",
        "                  print_val += f\"validation_running_loss : {(loss.item()):.6f}\\t\"\n",
        "                  print_val += f\"validation_running_corrects : {torch.sum(preds == labels.data)}\\t\"  \n",
        "                  print_val += f\"validation_total_corrects : {running_corrects}\\t\"  \n",
        "                  sys.stdout.write('\\r' + str(print_val))\n",
        "\n",
        "              epoch_val_loss = running_loss / len(testloaders)\n",
        "              epoch_val_acc = running_corrects.double().item() /len(testloaders.dataset)\n",
        "              all_validation_loss.append(epoch_val_loss)\n",
        "              all_validation_accuracy.append(epoch_val_acc)\n",
        "\n",
        "              print()\n",
        "              print(\"----------------------------Validation Summary-----------------\")\n",
        "              print('{} Validation Avg. Loss: {:.4f} Validation Avg. Acc: {:.4f}'.format(\n",
        "                    phase, epoch_val_loss, epoch_val_acc))\n",
        "              print(\"------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "          model.train()\n",
        "          \n",
        "          print(\"-------Start Model Save----\\n\\n\")\n",
        "          save_model(model_used,\n",
        "                     model_save_path,\n",
        "                     epoch, \n",
        "                     len(dataloaders),\n",
        "                     model,\n",
        "                     optimizer,\n",
        "                     IMAGE_SIZE,\n",
        "                     tranning_loss=all_tranning_loss,\n",
        "                     tranning_acc= all_tranning_accuracy,\n",
        "                     validation_loss=all_validation_loss,\n",
        "                     validation_acc=all_validation_accuracy,\n",
        "                     learning_rate=lernRate)\n",
        "          # break\n",
        "      \n",
        "    print(\"Complete Train\")\n",
        "          ## deep copy the model\n",
        "          # if phase == 'val' and epoch_acc > best_acc:\n",
        "          #       best_acc = epoch_acc\n",
        "          #       best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print()\n",
        "    \n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "    return "
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWHT-avGu4di"
      },
      "source": [
        "## Start Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tafnrtV5jdi"
      },
      "source": [
        "# 1 (26 - 1 * 28 * 28) (own) (FMNIST)\n",
        "# 1_2 (300 - 1 * 28 * 28) (own) (FMNIST)\n",
        "# 1_3 (300 - 1 * 28 * 28) (own) (FMNIST)\n",
        "\n",
        "# 2 (100 - 1 * 128 * 128) (FMNIST) \n",
        "# 2 (100 - 1 * 128 * 128) (own) \n",
        "# 2 (100 - 3 * 128 * 128) (own) \n",
        "\n",
        "# 3 (25 - 1 * 128 * 128) (FMNIST) \n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veOjFgCygkhm",
        "outputId": "ee52de74-a618-49fd-ffca-07e563307951"
      },
      "source": [
        "!mkdir 'EXP-3-COLOR-OWN'\n",
        "%cd '/content'"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory EXP-3-COLOR-OWN: File exists\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaKfUzXK0j2v"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "START=0\n",
        "model_save_path = './EXP-3-COLOR-OWN/' # must give `/` for the folder directory\n",
        "\n",
        "model_used= 'EXPERIMENT_MODEL_3_RSNET_COLOR'\n",
        "# model_ft = model_1\n",
        "# model_ft = model_1_2\n",
        "# model_ft = model_1_3\n",
        "# model_ft = model_2\n",
        "model_ft = model_3\n",
        "optimizer_ft = optimizer\n",
        "all_tranning_loss = [];\n",
        "all_validation_loss = [];\n",
        "all_tranning_accuracy = []; \n",
        "all_validation_accuracy = [];\n",
        "\n",
        "load_saved_model='/content/EXP-1-2-OWN/EXPERIMENT_MODEL_1_2-checkpoint-epoch-187.pt'\n",
        "p = Path(load_saved_model)\n",
        "if len(load_saved_model) > 1 and p.exists():\n",
        "  loadedModel = torch.load(load_saved_model, map_location=device)\n",
        "  model_used = loadedModel['MODEL_USED']\n",
        "  model_ft =  loadedModel['model_full']\n",
        "  model_ft.load_state_dict(loadedModel['model_state'])\n",
        "  optimizer_ft = loadedModel['optimizer_full']\n",
        "  optimizer_ft.load_state_dict(loadedModel['optimizer_state'] )\n",
        "  START = loadedModel['epoch'] + 1 \n",
        "  all_tranning_loss = loadedModel['tranning_loss'] \n",
        "  all_validation_loss = loadedModel['validation_loss'] \n",
        "  all_tranning_accuracy = loadedModel['tranning_acc'] \n",
        "  all_validation_accuracy = loadedModel['validation_acc']\n",
        "\n",
        "END=25\n",
        "trainloader= trainloader\n",
        "testloader = validationloader"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "4GBVkJFzuYRj",
        "outputId": "2f86d9b4-5131-49ea-9c71-0a99f7d0fdd1"
      },
      "source": [
        "train_model(START, END,model_used, model_save_path, model_ft, criterion, optimizer_ft, trainloader, testloader, \n",
        "            0.01,\n",
        "            all_tranning_loss, \n",
        "            all_validation_loss, \n",
        "            all_tranning_accuracy, \n",
        "            all_validation_accuracy)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/24\n",
            "----------\n",
            "Epoch: 0/24 Steps 0 \trunning_loss : 2.278082\trunning_corrects : 2\ttotal_corrects : 2\t"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-c6f463c6fbac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mall_validation_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mall_tranning_accuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             all_validation_accuracy)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-63-499c3b126e32>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(start, end, model_used, model_save_path, model, criterion, optimizer, dataloaders, testloaders, lernRate, all_tranning_loss, all_validation_loss, all_tranning_accuracy, all_validation_accuracy)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuwKDsf70zJk"
      },
      "source": [
        "# !cp  '/content/drive/MyDrive/MY_COURSE/4.2/CSE-4238-Soft Computuing/Assignment 2/OWN/EXP-1-2-OWN.zip' './'\n",
        "# !unzip '/content/EXP-1-2-OWN.zip' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIwwFK_kDT2l"
      },
      "source": [
        "# !mkdir 'EXP-2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3VkXWLRJFHj"
      },
      "source": [
        "!zip -r 'EXP-3-COLOR-OWN.zip' 'EXP-3-COLOR-OWN' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3l0RHaGDcgN"
      },
      "source": [
        "# !mv '/content/EXPERIMENT_MODEL_2-checkpoint-epoch-26.pt' '/content/EXP-2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0xPnRY32xwf"
      },
      "source": [
        "# !rm -rf '/content/EXP-1/EXP-1/drive/MyDrive/MY_COURSE/4.2/CSE-4238-Soft Computuing/Assignment 2/EXP-1/drive'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf0H9JYOnlWX"
      },
      "source": [
        "!cp -rfv '/content/EXP-1-3-OWN.zip'  '/content/drive/Shareddrives/Test shared drive'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWIxDaG-E4Ro"
      },
      "source": [
        "!cp -rfv '/content/EXP-1-3-OWN.zip'  '/content/drive/MyDrive/MY_COURSE/4.2/CSE-4238-Soft Computuing/Assignment 2/OWN'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mce3NaDT_xR3"
      },
      "source": [
        "# class CNN(torch.nn.Module): \n",
        "#     def __init__(self):\n",
        "#         super(CNN, self).__init__()\n",
        "\n",
        "#         #initializing 4 convolution layer\n",
        "#         self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3)\n",
        "#         self.conv2 = torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n",
        "#         self.conv3 = torch.nn.Conv2d(in_channels=32, out_channels=128, kernel_size=3)\n",
        "#         self.conv4 = torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4)\n",
        "\n",
        "#         #initializing dropout \n",
        "#         self.dropout = torch.nn.Dropout(0.2)\n",
        "\n",
        "#         #initializing dropout \n",
        "#         self.pool= torch.nn.MaxPool2d(2,2)\n",
        " \n",
        "#         #initializing linear \n",
        "#         self.fc1 = torch.nn.Linear(256 * 5 * 5, 512)\n",
        "#         self.fc2 = torch.nn.Linear(512, 64)\n",
        "#         self.fc3 = torch.nn.Linear(64, 32)\n",
        "#         self.fc4 = torch.nn.Linear(32, 10)\n",
        " \n",
        "#     def forward(self, x):\n",
        "#         x = self.pool(torch.nn.functional.relu(self.conv1(x))) #sending input into 1st convolution layer,then to relu ,then to pooling layer , param = ((3*3*3)+1)*16 = 448\n",
        "#         x = self.pool(torch.nn.functional.relu(self.conv2(x))) #sending previous output into 2nd convolution layer,then to relu ,then to pooling layer, param = ((3*3*16)+1)*32 = 4640 \n",
        "#         x = self.dropout(x) #dropout unnecessary output\n",
        "#         x = self.pool(torch.nn.functional.relu(self.conv3(x)))\n",
        "#         x = self.pool(torch.nn.functional.relu(self.conv4(x)))\n",
        "#         x = self.dropout(x)\n",
        "#         x = x.view(-1, 256 * 5 * 5) # for flatten layer\n",
        "#         x = torch.nn.functional.relu(self.fc1(x))\n",
        "#         x = torch.nn.functional.relu(self.fc2(x))\n",
        "#         x = torch.nn.functional.relu(self.fc3(x))\n",
        "#         x = self.fc4(x)\n",
        "#         return x\n",
        "        \n",
        "# #select CPU or GPU as a device\n",
        "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# device\n",
        "# model = CNN().to(device)\n",
        "# print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFMzERf9EoFP"
      },
      "source": [
        "# Classification Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbWZXc4qFcEe"
      },
      "source": [
        "# import some importent library or packages \n",
        "import glob,sys,os\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import numpy as np\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "import zipfile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "import torch.optim as optim\n",
        "import time,sys\n",
        "import copy\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY3X4zCJE06P"
      },
      "source": [
        "## Download Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtzdDyOPEy8H"
      },
      "source": [
        "# !gdown --id 1bhegwtIpwAYCsnptXoYrSEmuEObyMq4Y ## insecptionV3 without as output\n",
        "# https://drive.google.com/file/d/1bwC7yVaHFsLmyiQFExewVkt33DeuhLpm/view?usp=sharing\n",
        "# https://drive.google.com/file/d/10jt7Oe7RrFtNWvsY1eQTRPS2nCNqmCnB/view?usp=sharing\n",
        "# https://drive.google.com/file/d/10s3Cln4xLViOizFxe1HHkSYKjq7_A_3X/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1GQpI5HSHF_cg8mvBsBxc89ohSa5fDqEv/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1-8AKgpmaqLDT5K6CGmfZ-SrgC5vtxqYG/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1VLPrGSLzQBbL_G1rTufcs7EmMZ_YJGH-/view?usp=sharing\n",
        "\n",
        "# https://drive.google.com/file/d/1IxM0bieCLAaWWe_r67sd_Jz-oS8qXVfN/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1-8x2xQQ0NOxpkF1MMe0k2Sz-PjnAS8Lc/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1LwOS_LP56Pm1xcbjXRzf_EYClpz167PK/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1qpsII4q2Xk1mdSrS3KW4K8Pe7ugY8rs3/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1-7a44W76z-joINkLC_0ffcdBkPkRY_y8/view?usp=sharing\n",
        "\n",
        "# https://drive.google.com/file/d/1PBTq5xRgadmgc_MocZIHGhCFY80QBAg2/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1-92tqY9DwsAHgUikt5B_9dMPc7Oq1sWH/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1VzWoWbjjMeptGSLgqqBxSdwiQ28CFmp9/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1K73sdUVBVx29VX8cBABH0AInev920sT0/view?usp=sharing\n",
        "!gdown --id 1K73sdUVBVx29VX8cBABH0AInev920sT0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8-5cwM59_sX"
      },
      "source": [
        "!rm -rf  '/content/MODEL'\n",
        "!mkdir '/content/MODEL'\n",
        "!unzip -u '/content/EXP-3-FMINST.zip' -d '/content/MODEL'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfKfNDX7E3cG"
      },
      "source": [
        "## Model Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZjk0Zk3-kp2"
      },
      "source": [
        "class LIN_MODEL(torch.nn.Module): \n",
        "    def __init__(self,outDim):\n",
        "        super(LIN_MODEL, self).__init__()\n",
        "\n",
        "        self.fc_1 = torch.nn.Linear(784, 90)\n",
        "        self.fc_2 = torch.nn.Linear(90, 50)\n",
        "        self.fc_3 = torch.nn.Linear(50, 30)\n",
        "        self.fc_4 = torch.nn.Linear(30, 18)\n",
        "        self.fc_5 = torch.nn.Linear(18, 12)\n",
        "        self.fc_6 = torch.nn.Linear(12, outDim)\n",
        "\n",
        "        self.linear = torch.nn.Linear(784, outDim)\n",
        " \n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(-1, 28 * 28) \n",
        "        x = torch.nn.functional.relu(self.fc_1(x))\n",
        "        x = torch.nn.functional.softmax(self.fc_2(x),dim=1)\n",
        "        x = torch.nn.functional.relu(self.fc_3(x))\n",
        "        x = torch.nn.functional.softmax(self.fc_4(x),dim=1)\n",
        "        x = torch.nn.functional.relu(self.fc_5(x))\n",
        "        x = self.fc_6(x)\n",
        "        # x = torch.nn.functional.softmax(self.linear(x),dim=1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LIN_MODEL_2(torch.nn.Module): \n",
        "    def __init__(self,outDim):\n",
        "        super(LIN_MODEL_2, self).__init__()\n",
        "\n",
        "        self.fc_1 = torch.nn.Linear(784, 742)\n",
        "        self.fc_2 = torch.nn.Linear(742, 621)\n",
        "        self.fc_3 = torch.nn.Linear(621, 510)\n",
        "        self.fc_6 = torch.nn.Linear(510, outDim)\n",
        " \n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(-1, 28 * 28) \n",
        "        x = torch.nn.functional.relu(self.fc_1(x))\n",
        "        x = torch.nn.functional.relu(self.fc_2(x))\n",
        "        x = torch.nn.functional.relu(self.fc_3(x))\n",
        "        x = torch.nn.functional.relu(self.fc_6(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class LIN_MODEL_3(torch.nn.Module): \n",
        "    def __init__(self,outDim):\n",
        "        super(LIN_MODEL_3, self).__init__()\n",
        "\n",
        "        self.fc_1 = torch.nn.Linear(784, 512)\n",
        "        self.fc_2 = torch.nn.Linear(512, 256)\n",
        "        self.fc_6 = torch.nn.Linear(256, outDim)\n",
        " \n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(-1, 28 * 28) \n",
        "        x = torch.nn.functional.relu(self.fc_1(x))\n",
        "        x = torch.nn.functional.relu(self.fc_2(x))\n",
        "        x = torch.nn.functional.relu(self.fc_6(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "class CNN(torch.nn.Module): \n",
        "    def __init__(self ,outDim):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        #initializing convolution layer\n",
        "        # self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3)\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3)\n",
        "        self.conv2 = torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n",
        "\n",
        "        #initializing dropout \n",
        "        self.dropout = torch.nn.Dropout(0.2)\n",
        "\n",
        "        #initializing dropout \n",
        "        self.pool= torch.nn.MaxPool2d(2,2)\n",
        " \n",
        "        #initializing linear\n",
        "        self.fc1 = torch.nn.Linear(32* 30* 30, 512)\n",
        "        self.fc2 = torch.nn.Linear(512,64)\n",
        "        self.fc3 = torch.nn.Linear(64,10)\n",
        "\n",
        " \n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.nn.functional.relu(self.conv1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.pool(torch.nn.functional.relu(self.conv2(x))) \n",
        "        x = self.dropout(x)\n",
        "        x = x.view(-1, 32* 30* 30) \n",
        "        # print(x.shape)\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        x = torch.nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m01AjAaUE4on"
      },
      "source": [
        "MODEL_LOAD_PATH = '/content/MODEL/EXP-3-FMINST/EXPERIMENT_MODEL_3-checkpoint-epoch-24.pt'\n",
        "model = torch.load(MODEL_LOAD_PATH,map_location='cpu')\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZz2b0suFmDm"
      },
      "source": [
        "## All the loss and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDQxaN7NFiQG"
      },
      "source": [
        "_tranning_loss = model['tranning_loss']\n",
        "_tranning_acc = model['tranning_acc']\n",
        "_validation_loss = model['validation_loss']\n",
        "_validation_acc = model['validation_acc']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RRZhE64Fi0-"
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"LOSS Exp-2 Model-3 FMINIST\")\n",
        "plt.plot(_tranning_loss,label=\"Tranning Loss\")\n",
        "plt.plot(_validation_loss,label=\"Validation Loss\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DMcblXtFqOp"
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Accuracy Exp-2 Model-3 FMINIST\")\n",
        "plt.plot(_tranning_acc,label=\"Tranning Accuracy\")\n",
        "plt.plot(_validation_acc,label=\"Validation Accuracy\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txd__RCzFn8P"
      },
      "source": [
        "## Load test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSTermOW_lum"
      },
      "source": [
        "\n",
        "IMAGE_SIZE = model['image_size'] ## set image height width to 128x128\n",
        "transform = transforms.Compose([\n",
        "                                # transforms.ToPILImage(),\n",
        "\n",
        "                                ## this is only for when model is 1\n",
        "                                transforms.Grayscale(), \n",
        "                                \n",
        "                                # transforms.RandomRotation(20,expand=True), ## adding random rotation 20deg\n",
        "                                # torchvision.transforms.ColorJitter(hue=.05, saturation=.05), ## adding color filter\n",
        "                                # transforms.RandomVerticalFlip(), ## adding vertical flip\n",
        "                                # transforms.RandomHorizontalFlip(), ## adding horizontal flip\n",
        "                                transforms.Resize(IMAGE_SIZE),  ## image resize\n",
        "                                transforms.CenterCrop(IMAGE_SIZE), ## image center crop\n",
        "                                transforms.ToTensor(), ## array converted into torch tensor and then divided by 255 (1.0/255)\n",
        "                                # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                               ])\n",
        "trainDataset = torchvision.datasets.FashionMNIST(root='./data', \n",
        "                            train=True, \n",
        "                            transform=transform,  # Normalize the image to [0-1] from [0-255]\n",
        "                            download=True)\n",
        "\n",
        "validDataSet = torchvision.datasets.FashionMNIST(root='./data', \n",
        "                           train=False, \n",
        "                           transform=transform)\n",
        "\n",
        "'''\n",
        "MAKING DATASET ITERABLE\n",
        "'''\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(dataset=trainDataset, \n",
        "                                           batch_size=1, \n",
        "                                           shuffle=True)   # It's better to shuffle the whole training dataset! \n",
        "\n",
        "testloader = torch.utils.data.DataLoader(dataset=validDataSet, \n",
        "                                          batch_size=1, \n",
        "                                          shuffle=False)  \n",
        "\n",
        "print(\"Length of the trainloader:\", len(trainloader ) * 1)\n",
        "print(\"Length of the validationloader:\", len(testloader ) * 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0oIxtF4FofN"
      },
      "source": [
        "# # BATCH_SIZE = 32 ## number of image batch size \n",
        "# base_dir = '/content/PROCESSED_DATASET-170104028' \n",
        "# TEST_SIZE = 0.2\n",
        "# IMAGE_SIZE = model['image_size'] ## set image height width to 128x128\n",
        "# transform = transforms.Compose([\n",
        "#                                 # transforms.ToPILImage(),\n",
        "\n",
        "#                                 ## this is only for when model is 1\n",
        "#                                 transforms.Grayscale(), \n",
        "                                \n",
        "#                                 # transforms.RandomRotation(20,expand=True), ## adding random rotation 20deg\n",
        "#                                 # torchvision.transforms.ColorJitter(hue=.05, saturation=.05), ## adding color filter\n",
        "#                                 # transforms.RandomVerticalFlip(), ## adding vertical flip\n",
        "#                                 # transforms.RandomHorizontalFlip(), ## adding horizontal flip\n",
        "#                                 transforms.Resize(IMAGE_SIZE),  ## image resize\n",
        "#                                 transforms.CenterCrop(IMAGE_SIZE), ## image center crop\n",
        "#                                 transforms.ToTensor(), ## array converted into torch tensor and then divided by 255 (1.0/255)\n",
        "#                                 # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "#                                ])\n",
        "\n",
        "# #load training dataset \n",
        "# dataset = torchvision.datasets.ImageFolder(base_dir, transform=transform)\n",
        "# n = len(dataset) \n",
        "# n_test = int(TEST_SIZE * n) # 10% validation\n",
        "# trainDataset, validDataSet = torch.utils.data.random_split(dataset,[n - n_test,n_test]) #random split dataset\n",
        "# trainloader = torch.utils.data.DataLoader(trainDataset, batch_size=1, shuffle=True, pin_memory=True,) \n",
        "# testloader = torch.utils.data.DataLoader(validDataSet, batch_size=1, shuffle=True, pin_memory=True,) \n",
        "# print(\"Length of the trainloader:\", len(trainloader ) * 1)\n",
        "# print(\"Length of the validationloader:\", len(testloader ) * 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5XTgUzpF7nA"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKoYidVPF9zI"
      },
      "source": [
        "## Load model with state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA0PI1auF9ZG"
      },
      "source": [
        "preTrainedModel = model['model_full']\n",
        "preTrainedModel.load_state_dict( model['model_state'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46IMMmn1GCOG"
      },
      "source": [
        "## Model prediction  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uomJnmc9GAgl"
      },
      "source": [
        "y_true_tensor = torch.tensor([])\n",
        "y_pred_tensor = torch.tensor([])\n",
        "preTrainedModel.eval()\n",
        "for i,(inputs, labels) in enumerate(testloader):\n",
        "\n",
        "  inputs, labels = inputs.to(device), labels.to(device)\n",
        "  outputs = preTrainedModel(inputs)\n",
        "  _, preds = torch.max(outputs, 1)\n",
        "  y_true_tensor = torch.cat((y_true_tensor,labels))\n",
        "  y_pred_tensor = torch.cat((y_pred_tensor,preds))\n",
        "  print(i)\n",
        "  # break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEUbIeiLGEwe"
      },
      "source": [
        "# Convert tensor to list\n",
        "y_true = y_true_tensor.tolist()\n",
        "y_pred = y_pred_tensor.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6zlRbKtGLXo"
      },
      "source": [
        "## Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z79lQHhGJ9N"
      },
      "source": [
        "matrice = confusion_matrix(y_true,y_pred)\n",
        "matrice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-G56ntEGMiu"
      },
      "source": [
        "\n",
        "## FOR F-MNIST\n",
        "target_names = list(validDataSet.class_to_idx.keys())\n",
        "df_cm = pd.DataFrame(matrice,columns=target_names,index=target_names)\n",
        "plt.figure(figsize=(15,15))\n",
        "sn.heatmap(df_cm, annot=True,annot_kws={\"size\": 10},fmt='g',cmap='Blues',)\n",
        "plt.title(f\"Confusion matrix {model['MODEL_USED']}\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "## FOR OWN\n",
        "# target_names = list(dataset.class_to_idx.keys())\n",
        "# df_cm = pd.DataFrame(matrice,columns=target_names,index=target_names)\n",
        "# plt.figure(figsize=(15,15))\n",
        "# sn.heatmap(df_cm, annot=True,annot_kws={\"size\": 10},fmt='g',cmap='Blues',)\n",
        "# plt.title(f\"Confusion matrix {model['MODEL_USED']}\")\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CaphhnQGOBm"
      },
      "source": [
        "## Classification Report `recision,  recall,  f1-score`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNVWNb-jGQJF"
      },
      "source": [
        "## FOR FNIMST\n",
        "target_names = list(validDataSet.class_to_idx.keys())\n",
        "classify_report = classification_report(y_true, y_pred, target_names=target_names)\n",
        "print(classify_report)\n",
        "\n",
        "\n",
        "## FOR OWN\n",
        "# target_names = list(dataset.class_to_idx.keys())\n",
        "# classify_report = classification_report(y_true, y_pred, target_names=target_names)\n",
        "# print(classify_report)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}