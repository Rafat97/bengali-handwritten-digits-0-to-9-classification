{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSE 4238 - Soft Computing Lab - Assignment 2 - Exp-1 .ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3J654otXDKeT",
        "nH_5Fjucw7Vs",
        "v1Q9jiYY6SwS",
        "CZokUlmS-QlQ",
        "Muzux96C5B4p",
        "ITcWE8G--CQq",
        "xpJj9oCT_wEx",
        "vu-fq2VRtEWs",
        "z4ZOi5mGuLcD",
        "0w0oLNMdu0Jr",
        "bWHT-avGu4di"
      ],
      "authorship_tag": "ABX9TyMETLnwheYV6BwKITdIFAg1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rafat97/bengali-handwritten-digits-0-to-9-classification/blob/master/CSE_4238_Soft_Computing_Lab_Assignment_2_Exp_1_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NcKrXxBDEtr"
      },
      "source": [
        "# Notebook Author info\n",
        "<center>\n",
        "\n",
        "| |  |\n",
        "| ----------- | ----------- |\n",
        "| ![Emdadul Haque Rafat](https://rafat97.github.io/static/c3688eb99d1fef50023a121e3abc5fa6/e8044/my-image.jpg)      | `Name:` Emdadul Haque<br /><br /> `Professional Status:` Student of Computer Science and Engineering <br /><br /> `Email:` rafathaque1997@gmail.com <br /><br /> `Website :` https://rafat97.github.io/ <br /><br />`Github:` https://github.com/Rafat97 <br /><br /> `Linkedin:` https://www.linkedin.com/in/rafat-haque-173131139/   |\n",
        "\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J654otXDKeT"
      },
      "source": [
        "# Drive mount code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvebfabg7Rll"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH_5Fjucw7Vs"
      },
      "source": [
        "# Data Load From zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlUs8I-8bvII"
      },
      "source": [
        "# import some importent library or packages \n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import time,sys\n",
        "import copy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import pathlib\n",
        "import zipfile\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "import torch.optim as optim\n",
        "import pathlib\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "!pip install torchviz\n",
        "from torchviz import make_dot, make_dot_from_trace\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "torch.manual_seed(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaKNziom4vdo"
      },
      "source": [
        "## Download preprocessed dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyR7nygy4chy"
      },
      "source": [
        "!gdown --id '1cml-H4UUJyY0hRoVeAynO8JjvWqa7FU1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMKBFffe458J"
      },
      "source": [
        "## Unzip the download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4KGVLdE4qXK"
      },
      "source": [
        "!unzip '/content/PROCESSED_DATASET-170104028.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQQQa_d35TTZ"
      },
      "source": [
        "## Remove the zip file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyZ6yOMA5PQJ"
      },
      "source": [
        "!rm '/content/PROCESSED_DATASET-170104028.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1Q9jiYY6SwS"
      },
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMbUWiw3Nj9J"
      },
      "source": [
        "# import some importent library or packages \n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import time,sys\n",
        "import copy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import pathlib\n",
        "import zipfile\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "import torch.optim as optim\n",
        "import pathlib\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "!pip install torchviz\n",
        "from torchviz import make_dot, make_dot_from_trace\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "torch.manual_seed(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZokUlmS-QlQ"
      },
      "source": [
        "# important variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVsnfKIp-P-Q"
      },
      "source": [
        "base_dir = '/content/PROCESSED_DATASET-170104028' \n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# 28(model-1), 128(model-2), 224(model-3),\n",
        "IMAGE_SIZE = 28\n",
        "LEARNING_RATE = 0.01\n",
        "TEST_SIZE = 0.2\n",
        "OUTPUT_DIM=10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Muzux96C5B4p"
      },
      "source": [
        "# Load Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLPbZAzwN0Vs"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "                                # transforms.ToPILImage(),\n",
        "\n",
        "                                ## this is only for when model is 1\n",
        "                                transforms.Grayscale(), \n",
        "                                \n",
        "                                # transforms.RandomRotation(20,expand=True), ## adding random rotation 20deg\n",
        "                                # torchvision.transforms.ColorJitter(hue=.05, saturation=.05), ## adding color filter\n",
        "                                # transforms.RandomVerticalFlip(), ## adding vertical flip\n",
        "                                # transforms.RandomHorizontalFlip(), ## adding horizontal flip\n",
        "                                transforms.Resize(IMAGE_SIZE),  ## image resize\n",
        "                                transforms.CenterCrop(IMAGE_SIZE), ## image center crop\n",
        "                                transforms.ToTensor(), ## array converted into torch tensor and then divided by 255 (1.0/255)\n",
        "                                # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                               ])\n",
        "\n",
        "#load training dataset \n",
        "dataset = torchvision.datasets.ImageFolder(base_dir, transform=transform)\n",
        "n = len(dataset) \n",
        "n_test = int(TEST_SIZE * n) # 10% validation\n",
        "trainDataset, validDataSet = torch.utils.data.random_split(dataset,[n - n_test,n_test]) #random split dataset\n",
        "trainloader = torch.utils.data.DataLoader(trainDataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,) \n",
        "validationloader = torch.utils.data.DataLoader(validDataSet, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,) \n",
        "print(\"Length of the trainloader:\", len(trainloader ) * BATCH_SIZE)\n",
        "print(\"Length of the validationloader:\", len(validationloader ) * BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Svy95MXy_5BB"
      },
      "source": [
        "\n",
        "# trainDataset = torchvision.datasets.FashionMNIST(root='./data', \n",
        "#                             train=True, \n",
        "#                             transform=transforms.ToTensor(),  # Normalize the image to [0-1] from [0-255]\n",
        "#                             download=True)\n",
        "\n",
        "# validDataSet = torchvision.datasets.FashionMNIST(root='./data', \n",
        "#                            train=False, \n",
        "#                            transform=transforms.ToTensor())\n",
        "\n",
        "# '''\n",
        "# MAKING DATASET ITERABLE\n",
        "# '''\n",
        "\n",
        "# trainloader = torch.utils.data.DataLoader(dataset=trainDataset, \n",
        "#                                            batch_size=BATCH_SIZE, \n",
        "#                                            shuffle=True)   # It's better to shuffle the whole training dataset! \n",
        "\n",
        "# validationloader = torch.utils.data.DataLoader(dataset=validDataSet, \n",
        "#                                           batch_size=BATCH_SIZE, \n",
        "#                                           shuffle=False)  \n",
        "\n",
        "# print(\"Length of the trainloader:\", len(trainloader ) * BATCH_SIZE)\n",
        "# print(\"Length of the validationloader:\", len(validationloader ) * BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITcWE8G--CQq"
      },
      "source": [
        "# Data basic info."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah_Uya58td5d"
      },
      "source": [
        "dataset.class_to_idx # class map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMKQzdDG7NZ5"
      },
      "source": [
        "print(dict(Counter(dataset.targets))) # count number of data in a class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM9GI3D17tUp"
      },
      "source": [
        "train_classes = [label for _, label in trainDataset]\n",
        "print(dict(Counter(train_classes)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4u7UWJl858x"
      },
      "source": [
        "val_classes = [label for _, label in validDataSet]\n",
        "print(dict(Counter(val_classes)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8e6dxJq-dwY"
      },
      "source": [
        "for image,label in trainloader:\n",
        "  plt.figure(figsize=(20,20))\n",
        "  grid_imge_gen = torchvision.utils.make_grid(image)\n",
        "  plt.imshow(grid_imge_gen.permute(1, 2, 0).cpu())\n",
        "  plt.title(\"Trainloader\")\n",
        "  plt.show()\n",
        "  print(label)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAnMIFYL-opY"
      },
      "source": [
        "for image,label in validationloader:\n",
        "  plt.figure(figsize=(20,20))\n",
        "  grid_imge_gen = torchvision.utils.make_grid(image)\n",
        "  plt.imshow(grid_imge_gen.permute(1, 2, 0).cpu())\n",
        "  plt.title(\"Validationloader\")\n",
        "  plt.show()\n",
        "  print(label)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpJj9oCT_wEx"
      },
      "source": [
        "# Model Creation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-tEKJ2uV8K0"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu-fq2VRtEWs"
      },
      "source": [
        "## Model 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13hvC1cTV6AN"
      },
      "source": [
        "'''\n",
        "Model creation \n",
        "'''\n",
        "\n",
        "class LIN_MODEL(torch.nn.Module): \n",
        "    def __init__(self,outDim):\n",
        "        super(LIN_MODEL, self).__init__()\n",
        "\n",
        "        self.fc_1 = torch.nn.Linear(784, 90)\n",
        "        self.fc_2 = torch.nn.Linear(90, 50)\n",
        "        self.fc_3 = torch.nn.Linear(50, 30)\n",
        "        self.fc_4 = torch.nn.Linear(30, 18)\n",
        "        self.fc_5 = torch.nn.Linear(18, 12)\n",
        "        self.fc_6 = torch.nn.Linear(12, outDim)\n",
        "\n",
        "        self.linear = torch.nn.Linear(784, outDim)\n",
        " \n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(-1, 28 * 28) \n",
        "        x = torch.nn.functional.relu(self.fc_1(x))\n",
        "        x = torch.nn.functional.softmax(self.fc_2(x),dim=1)\n",
        "        x = torch.nn.functional.relu(self.fc_3(x))\n",
        "        x = torch.nn.functional.softmax(self.fc_4(x),dim=1)\n",
        "        x = torch.nn.functional.relu(self.fc_5(x))\n",
        "        x = self.fc_6(x)\n",
        "        # x = torch.nn.functional.softmax(self.linear(x),dim=1)\n",
        "        return x\n",
        "        \n",
        "\n",
        "model_1 = LIN_MODEL(OUTPUT_DIM).to(device)\n",
        "\n",
        "summary( model_1, input_size=(1, 28, 28))\n",
        "# select CPU or GPU as a device\n",
        "print(model_1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykGaF2ILrB-H"
      },
      "source": [
        "x = torch.randn(BATCH_SIZE,1,28,28).to(device)\n",
        "make_dot(model_1(x), params=dict(model_1.named_parameters()), show_attrs=True, show_saved=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d65L4VNBmWY"
      },
      "source": [
        "make_dot(model_1(x), params=dict(model_1.named_parameters()), show_attrs=True, show_saved=True).render(\"Model-1\", format=\"png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lKpdU7gtOcS"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_1.parameters(),lr=LEARNING_RATE, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dbj7OlK0EKb9"
      },
      "source": [
        "# Start Tranning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4ZOi5mGuLcD"
      },
      "source": [
        "## Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py448GSDuJTL"
      },
      "source": [
        "def save_model(\n",
        "               MODEL_USED,\n",
        "               SAVEPATH,\n",
        "               epoch, \n",
        "               batch_size, \n",
        "               model,\n",
        "               optimizer,\n",
        "               image_size,\n",
        "               tranning_loss=[],\n",
        "               tranning_acc=[],\n",
        "               validation_loss=[],\n",
        "               validation_acc=[],\n",
        "               learning_rate=0.001,\n",
        "               meta_data=None):\n",
        "  SAVEPATH += f\"{MODEL_USED}-checkpoint-epoch-{epoch}.pt\"\n",
        "  save_obj = {\n",
        "       'MODEL_USED':MODEL_USED,\n",
        "       'batch_size':batch_size,\n",
        "       'epoch': epoch,\n",
        "       'model_full': model,\n",
        "       'optimizer_full': optimizer,\n",
        "       'model_state': model.state_dict(),\n",
        "       'optimizer_state': optimizer.state_dict(),\n",
        "       'image_size': image_size,\n",
        "       'tranning_loss': tranning_loss,\n",
        "       'tranning_acc': tranning_acc,\n",
        "       'validation_loss': validation_loss,\n",
        "       'validation_acc': validation_acc,\n",
        "       'learning_rate':learning_rate,\n",
        "       'meta_data':meta_data\n",
        "       }\n",
        "\n",
        "  torch.save(save_obj, SAVEPATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w0oLNMdu0Jr"
      },
      "source": [
        "## Train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjWXGOHiuTnr"
      },
      "source": [
        "def train_model(start, end, \n",
        "                model_used ,\n",
        "                model_save_path, \n",
        "                model, \n",
        "                criterion, \n",
        "                optimizer, \n",
        "                dataloaders,\n",
        "                testloaders , \n",
        "                lernRate=0.001,\n",
        "                all_tranning_loss=[], all_validation_loss=[], all_tranning_accuracy=[], all_validation_accuracy=[]):\n",
        "    since = time.time()\n",
        "    num_epochs = end\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    phase = 'train'\n",
        "    steps = 0\n",
        "\n",
        "    for epoch in range(start,num_epochs):\n",
        "          model.train()\n",
        "          phase = 'tranning'\n",
        "          print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "          print('-' * 10)\n",
        "          running_loss = 0.0\n",
        "          running_corrects = 0\n",
        "\n",
        "          for i,(inputs, labels) in enumerate(dataloaders):\n",
        "                \n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # running_loss += loss.item() * inputs.size(0)\n",
        "                running_loss += loss.item()\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                \n",
        "\n",
        "                print_val = f\"Epoch: {epoch}/{num_epochs-1} Steps {steps} \\t\"\n",
        "                print_val += f\"running_loss : {(loss.item()):.6f}\\t\"\n",
        "                print_val += f\"running_corrects : {torch.sum(preds == labels.data)}\\t\"  \n",
        "                print_val += f\"total_corrects : {running_corrects}\\t\"  \n",
        "                sys.stdout.write('\\r' + str(print_val))\n",
        "                steps += 1\n",
        "          \n",
        "          \n",
        "          steps = 0\n",
        "          epoch_loss = running_loss / len(dataloaders)\n",
        "          epoch_acc = running_corrects.double().item() /len(dataloaders.dataset)\n",
        "          all_tranning_loss.append(loss.item())\n",
        "          all_tranning_accuracy.append(epoch_acc)\n",
        "          \n",
        "          \n",
        "          print(\"\\n\")\n",
        "          print(\"----------------------------Tranning Summary----------------------\")\n",
        "          print('{} Tranning Avg. Loss: {:.4f} Tranning Avg. Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "          print(\"-\"*70)\n",
        "          print(\"\\n\")\n",
        "\n",
        "          print(\"Start Validation\")\n",
        "          model.eval()\n",
        "          with torch.no_grad():\n",
        "              phase = \"validation\"\n",
        "              running_loss = 0.0\n",
        "              running_corrects = 0\n",
        "              for i,(inputs, labels) in enumerate(testloaders):\n",
        "                  inputs, labels = inputs.to(device), labels.to(device)\n",
        "                  outputs = model(inputs)\n",
        "                  _, preds = torch.max(outputs, 1)\n",
        "                  loss = criterion(outputs, labels)\n",
        "                    \n",
        "                  running_loss += loss.item()\n",
        "                  running_corrects += torch.sum(preds == labels.data)  \n",
        "\n",
        "                  print_val = f\"Steps {i} \\t\"\n",
        "                  print_val += f\"validation_running_loss : {(loss.item()):.6f}\\t\"\n",
        "                  print_val += f\"validation_running_corrects : {torch.sum(preds == labels.data)}\\t\"  \n",
        "                  print_val += f\"validation_total_corrects : {running_corrects}\\t\"  \n",
        "                  sys.stdout.write('\\r' + str(print_val))\n",
        "\n",
        "              epoch_val_loss = running_loss / len(testloaders)\n",
        "              epoch_val_acc = running_corrects.double().item() /len(testloaders.dataset)\n",
        "              all_validation_loss.append(epoch_val_loss)\n",
        "              all_validation_accuracy.append(epoch_val_acc)\n",
        "\n",
        "              print()\n",
        "              print(\"----------------------------Validation Summary-----------------\")\n",
        "              print('{} Validation Avg. Loss: {:.4f} Validation Avg. Acc: {:.4f}'.format(\n",
        "                    phase, epoch_val_loss, epoch_val_acc))\n",
        "              print(\"------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "          model.train()\n",
        "          \n",
        "          print(\"-------Start Model Save----\\n\\n\")\n",
        "          save_model(model_used,\n",
        "                     model_save_path,\n",
        "                     epoch, \n",
        "                     len(dataloaders),\n",
        "                     model,\n",
        "                     optimizer,\n",
        "                     IMAGE_SIZE,\n",
        "                     tranning_loss=all_tranning_loss,\n",
        "                     tranning_acc= all_tranning_accuracy,\n",
        "                     validation_loss=all_validation_loss,\n",
        "                     validation_acc=all_validation_accuracy,\n",
        "                     learning_rate=lernRate)\n",
        "          # break\n",
        "      \n",
        "    print(\"Complete Train\")\n",
        "          ## deep copy the model\n",
        "          # if phase == 'val' and epoch_acc > best_acc:\n",
        "          #       best_acc = epoch_acc\n",
        "          #       best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print()\n",
        "    \n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "    return "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWHT-avGu4di"
      },
      "source": [
        "## Start Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tafnrtV5jdi"
      },
      "source": [
        "# 1 (26 - 1 * 28 * 28) (own) (FMNIST)\n",
        "# 1_2 (300 - 1 * 28 * 28) (own) (FMNIST)\n",
        "# 1_3 (300 - 1 * 28 * 28) (own) (FMNIST)\n",
        "\n",
        "# 2 (100 - 1 * 128 * 128) (FMNIST) \n",
        "# 2 (100 - 1 * 128 * 128) (own) \n",
        "# 2 (100 - 3 * 128 * 128) (own) \n",
        "\n",
        "# 3 (25 - 1 * 128 * 128) (FMNIST) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veOjFgCygkhm"
      },
      "source": [
        "!mkdir 'EXP-3-COLOR-OWN'\n",
        "%cd '/content'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaKfUzXK0j2v"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "START=0\n",
        "model_save_path = './EXP-3-COLOR-OWN/' # must give `/` for the folder directory\n",
        "\n",
        "model_used= 'EXPERIMENT_MODEL_3_RSNET_COLOR'\n",
        "model_ft = model_1\n",
        "# model_ft = model_1_2\n",
        "# model_ft = model_1_3\n",
        "# model_ft = model_2\n",
        "# model_ft = model_3\n",
        "optimizer_ft = optimizer\n",
        "all_tranning_loss = [];\n",
        "all_validation_loss = [];\n",
        "all_tranning_accuracy = []; \n",
        "all_validation_accuracy = [];\n",
        "\n",
        "load_saved_model='/content/EXP-1-2-OWN/EXPERIMENT_MODEL_1_2-checkpoint-epoch-187.pt'\n",
        "p = Path(load_saved_model)\n",
        "if len(load_saved_model) > 1 and p.exists():\n",
        "  loadedModel = torch.load(load_saved_model, map_location=device)\n",
        "  model_used = loadedModel['MODEL_USED']\n",
        "  model_ft =  loadedModel['model_full']\n",
        "  model_ft.load_state_dict(loadedModel['model_state'])\n",
        "  optimizer_ft = loadedModel['optimizer_full']\n",
        "  optimizer_ft.load_state_dict(loadedModel['optimizer_state'] )\n",
        "  START = loadedModel['epoch'] + 1 \n",
        "  all_tranning_loss = loadedModel['tranning_loss'] \n",
        "  all_validation_loss = loadedModel['validation_loss'] \n",
        "  all_tranning_accuracy = loadedModel['tranning_acc'] \n",
        "  all_validation_accuracy = loadedModel['validation_acc']\n",
        "\n",
        "END=25\n",
        "trainloader= trainloader\n",
        "testloader = validationloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GBVkJFzuYRj"
      },
      "source": [
        "train_model(START, END,model_used, model_save_path, model_ft, criterion, optimizer_ft, trainloader, testloader, 0.01,\n",
        "            all_tranning_loss, \n",
        "            all_validation_loss, \n",
        "            all_tranning_accuracy, \n",
        "            all_validation_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuwKDsf70zJk"
      },
      "source": [
        "# !cp  '/content/drive/MyDrive/MY_COURSE/4.2/CSE-4238-Soft Computuing/Assignment 2/OWN/EXP-1-2-OWN.zip' './'\n",
        "# !unzip '/content/EXP-1-2-OWN.zip' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIwwFK_kDT2l"
      },
      "source": [
        "# !mkdir 'EXP-2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3VkXWLRJFHj"
      },
      "source": [
        "!zip -r 'EXP-3-COLOR-OWN.zip' 'EXP-3-COLOR-OWN' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3l0RHaGDcgN"
      },
      "source": [
        "# !mv '/content/EXPERIMENT_MODEL_2-checkpoint-epoch-26.pt' '/content/EXP-2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0xPnRY32xwf"
      },
      "source": [
        "# !rm -rf '/content/EXP-1/EXP-1/drive/MyDrive/MY_COURSE/4.2/CSE-4238-Soft Computuing/Assignment 2/EXP-1/drive'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf0H9JYOnlWX"
      },
      "source": [
        "!cp -rfv '/content/EXP-1-3-OWN.zip'  '/content/drive/Shareddrives/Test shared drive'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWIxDaG-E4Ro"
      },
      "source": [
        "!cp -rfv '/content/EXP-1-3-OWN.zip'  '/content/drive/MyDrive/MY_COURSE/4.2/CSE-4238-Soft Computuing/Assignment 2/OWN'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mce3NaDT_xR3"
      },
      "source": [
        "# class CNN(torch.nn.Module): \n",
        "#     def __init__(self):\n",
        "#         super(CNN, self).__init__()\n",
        "\n",
        "#         #initializing 4 convolution layer\n",
        "#         self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3)\n",
        "#         self.conv2 = torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n",
        "#         self.conv3 = torch.nn.Conv2d(in_channels=32, out_channels=128, kernel_size=3)\n",
        "#         self.conv4 = torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4)\n",
        "\n",
        "#         #initializing dropout \n",
        "#         self.dropout = torch.nn.Dropout(0.2)\n",
        "\n",
        "#         #initializing dropout \n",
        "#         self.pool= torch.nn.MaxPool2d(2,2)\n",
        " \n",
        "#         #initializing linear \n",
        "#         self.fc1 = torch.nn.Linear(256 * 5 * 5, 512)\n",
        "#         self.fc2 = torch.nn.Linear(512, 64)\n",
        "#         self.fc3 = torch.nn.Linear(64, 32)\n",
        "#         self.fc4 = torch.nn.Linear(32, 10)\n",
        " \n",
        "#     def forward(self, x):\n",
        "#         x = self.pool(torch.nn.functional.relu(self.conv1(x))) #sending input into 1st convolution layer,then to relu ,then to pooling layer , param = ((3*3*3)+1)*16 = 448\n",
        "#         x = self.pool(torch.nn.functional.relu(self.conv2(x))) #sending previous output into 2nd convolution layer,then to relu ,then to pooling layer, param = ((3*3*16)+1)*32 = 4640 \n",
        "#         x = self.dropout(x) #dropout unnecessary output\n",
        "#         x = self.pool(torch.nn.functional.relu(self.conv3(x)))\n",
        "#         x = self.pool(torch.nn.functional.relu(self.conv4(x)))\n",
        "#         x = self.dropout(x)\n",
        "#         x = x.view(-1, 256 * 5 * 5) # for flatten layer\n",
        "#         x = torch.nn.functional.relu(self.fc1(x))\n",
        "#         x = torch.nn.functional.relu(self.fc2(x))\n",
        "#         x = torch.nn.functional.relu(self.fc3(x))\n",
        "#         x = self.fc4(x)\n",
        "#         return x\n",
        "        \n",
        "# #select CPU or GPU as a device\n",
        "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# device\n",
        "# model = CNN().to(device)\n",
        "# print(model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}